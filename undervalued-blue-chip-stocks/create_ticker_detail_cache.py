# -*- coding: utf-8 -*-
"""
build_details_cache_fully_optimized.py

ğŸš€ ì™„ì „ ìµœì í™” ë²„ì „:
1. PER ë°ì´í„° ê³„ì‚° ë¡œì§ ê°•í™” (ì—¬ëŸ¬ ë°©ë²•ë¡  ì ìš©)
2. ìµœì‹  íŠ¸ë Œë“œ ì§€í‘œ ì¶”ê°€ (RSI, MACD, ë³¼ë¦°ì €ë°´ë“œ, 52ì£¼ ê³ ì €ê°€ ë“±)
3. ì¬ë¬´ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ
4. ì„±ì¥ì„± ì§€í‘œ ì¶”ê°€
5. â­ OHLCV í”„ë¦¬ë¡œë“œ ë³‘ë ¬í™” (2-3ë°° ë¹ ë¦„)
6. â­ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ë³‘ë ¬í™” (5-10ë°° ë¹ ë¦„)
7. â­ ì´ìƒì¹˜ ì²˜ë¦¬ ê°•í™”

âœ¨ ë°ì´í„° ìˆ˜ì§‘ ì•ˆì •ì„± ê°œì„ ì‚¬í•­:
1. ğŸ“Š OHLCV ë°ì´í„° ìˆ˜ì§‘ ì•ˆì •ì„± í–¥ìƒ:
   - ë¶€ë¶„ ì„±ê³µ ì¼€ì´ìŠ¤ ì²˜ë¦¬: ë°°ì¹˜ì—ì„œ ì¼ë¶€ ì‹¤íŒ¨ ì‹œ ëˆ„ë½ëœ í‹°ì»¤ë§Œ ê°œë³„ ë‹¤ìš´ë¡œë“œ
   - ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ ì™„í™”: 50ê°œ â†’ 20ê°œë¡œ ì™„í™”í•˜ì—¬ ë” ë§ì€ ì¢…ëª© ìˆ˜ì§‘ ê°€ëŠ¥
   - ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„ ë¡œì§: ë°°ì¹˜ ì™„ì „ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì¬ì‹œë„, ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ëˆ„ë½ë¶„ë§Œ ì¬ì‹œë„
   - ì¬ì‹œë„ íšŸìˆ˜: ë°°ì¹˜ 5íšŒ, ê°œë³„ 3íšŒ

2. ğŸ’¼ ìƒì„¸ ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ ê°œì„ :
   - ì¬ë¬´ì œí‘œ API ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (ê° API í˜¸ì¶œë‹¹ ìµœëŒ€ 3íšŒ)
   - info ì‹¤íŒ¨ ì‹œì—ë„ ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„
   - ê° ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨ ì‹œì—ë„ ë‹¤ë¥¸ ì§€í‘œëŠ” ê³„ì† ìˆ˜ì§‘
   - ì¬ë¬´ì œí‘œë³„ ë…ë¦½ì ì¸ ì—ëŸ¬ ì²˜ë¦¬

3. ğŸ” ì—ëŸ¬ ë¡œê¹… ë° ë””ë²„ê¹…:
   - ì „ì²´ ì—ëŸ¬ ì¶”ì  ì‹œìŠ¤í…œ ì¶”ê°€
   - ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ ìë™ ìƒì„±
   - ë°ì´í„° í’ˆì§ˆ í†µê³„ ìë™ ì¶œë ¥
   - VERBOSE_LOGGING ì˜µì…˜ìœ¼ë¡œ ìƒì„¸ ë¡œê·¸ ì œì–´

4. ğŸ›¡ï¸ ë°ì´í„° ê²€ì¦ ê°œì„ :
   - ê²€ì¦ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë¡œì§ ì¶”ê°€
   - ê°€ê²© ê²€ì¦ ë²”ìœ„ í™•ëŒ€
   - ê° í•„ë“œë³„ ë…ë¦½ì ì¸ ì—ëŸ¬ ì²˜ë¦¬ë¡œ ë¶€ë¶„ ë°ì´í„°ë¼ë„ ìˆ˜ì§‘
"""

import os, io, time, math, random, warnings, logging, requests
import pandas as pd, numpy as np, yfinance as yf
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
logging.getLogger("yfinance").setLevel(logging.CRITICAL)

# ===================== CONFIG =====================
CONFIG = {
    "UNIVERSE_SOURCE": "us_all",  # "us_all" | "sp500" | "custom"
    "CUSTOM_TICKERS": [],  # UNIVERSE_SOURCE="custom"ì¼ ë•Œ ì‚¬ìš©

    # ìºì‹œ ì¶œë ¥
    "OUT_BASENAME": "",  # ë¹„ìš°ë©´ ìë™: details_cache_{source}.csv
    "INCLUDE_EXCEL": True,

    "PRELOAD_PERIOD": "252d",  # 1ë…„ ë°ì´í„° (52ì£¼ ê³„ì‚°ìš©)
    "PRELOAD_CHUNK": 50,  # ë°°ì¹˜ í¬ê¸° (ì›ë˜ëŒ€ë¡œ ë³µì›)
    "BATCH_RETRIES": 5,  # ë°°ì¹˜ ì¬ì‹œë„
    "SINGLE_RETRIES": 3,  # ê°œë³„ ì¬ì‹œë„

    # â­ ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    "OHLCV_WORKERS": 1,  # OHLCV ë‹¤ìš´ë¡œë“œ ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜
    "DETAIL_FETCH_WORKERS": 1,  # ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜

    # ë””ë²„ê¹… ë° ë¡œê¹…
    "VERBOSE_LOGGING": False,  # Trueë¡œ ì„¤ì •í•˜ë©´ ìƒì„¸ ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥

    "YF_THREADS": False,
    "SLEEP_SEC": 0.1,  # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì—ëŠ” ì§§ê²Œ

    # ë„¤íŠ¸ì›Œí¬ ì„¤ì •
    "REQUEST_TIMEOUT": 60,
    "PROXY_SETTINGS": {},

    # ë¼ì´íŠ¸ ì»·
    "MIN_PRICE": 1.0,
    "MIN_DOLLAR_VOLUME": 900_000,

    # ìƒì„¸ ì¬ë¬´ í˜¸ì¶œ ëŒ€ìƒ ë²”ìœ„
    "DETAILED_TOP_K": 12000,
    "MAX_TICKERS": 12000,
    "UNIVERSE_OFFSET": 0,
    "SHUFFLE_UNIVERSE": True,

    # ë²„í•í˜• í•˜ë“œì»· ê¸°ë³¸ì„ 
    "MIN_MKTCAP": 800_000_000,

    # ìš”ì²­ í—¤ë”
    "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
}
# ==================================================

HEADERS = {"User-Agent": CONFIG["USER_AGENT"]}
HTTP_SESSION = requests.Session()
HTTP_SESSION.headers.update(HEADERS)

# ì„¸ì…˜ ì„¤ì •
session = requests.Session()
session.headers.update({"User-Agent": CONFIG["USER_AGENT"]})
if CONFIG["PROXY_SETTINGS"]:
    session.proxies.update(CONFIG["PROXY_SETTINGS"])


# ============== â­ ì´ìƒì¹˜ ê²€ì¦ í•¨ìˆ˜ ==============

def validate_numeric(value, min_val=None, max_val=None, allow_negative=False):
    """ìˆ«ì ê°’ ê²€ì¦ ë° ì´ìƒì¹˜ í•„í„°ë§"""
    if value is None:
        return None

    try:
        val = float(value)

        # NaN, Inf ì²´í¬
        if math.isnan(val) or math.isinf(val):
            return None

        # ìŒìˆ˜ ì²´í¬
        if not allow_negative and val < 0:
            return None

        # ë²”ìœ„ ì²´í¬
        if min_val is not None and val < min_val:
            return None
        if max_val is not None and val > max_val:
            return None

        return val
    except (TypeError, ValueError):
        return None


def validate_percentage(value, min_pct=-100, max_pct=1000):
    """í¼ì„¼í‹°ì§€ ê°’ ê²€ì¦ (-100% ~ 1000%)"""
    return validate_numeric(value, min_val=min_pct, max_val=max_pct, allow_negative=True)


def validate_ratio(value, min_ratio=0, max_ratio=1000):
    """ë¹„ìœ¨ ê°’ ê²€ì¦ (PER, PBR ë“±)"""
    return validate_numeric(value, min_val=min_ratio, max_val=max_ratio, allow_negative=False)


def validate_market_cap(value):
    """ì‹œê°€ì´ì•¡ ê²€ì¦ (ìµœì†Œ 100ë§Œë¶ˆ, ìµœëŒ€ 20ì¡°ë¶ˆ)"""
    return validate_numeric(value, min_val=1_000_000, max_val=20_000_000_000_000, allow_negative=False)


def validate_price(value):
    """ì£¼ê°€ ê²€ì¦ (0.01 ~ 100,000)"""
    return validate_numeric(value, min_val=0.01, max_val=100_000, allow_negative=False)


def validate_volume(value):
    """ê±°ë˜ëŸ‰ ê²€ì¦"""
    return validate_numeric(value, min_val=0, max_val=1e15, allow_negative=False)


# ============== ì—ëŸ¬ ë¡œê¹… ì„¤ì • ==============
ERROR_LOG = []  # ì—ëŸ¬ ì¶”ì ìš©


def log_error(context, ticker, error_msg):
    """ì—ëŸ¬ ë¡œê¹… í•¨ìˆ˜"""
    msg = f"[{context}] {ticker}: {error_msg}"
    ERROR_LOG.append(msg)
    if CONFIG.get("VERBOSE_LOGGING", False):
        print(f"âš ï¸  {msg}")


# ============== ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ë“¤ ==============

def calculate_rsi(prices, window=14):
    """RSI ê³„ì‚°"""
    try:
        if len(prices) < window + 1:
            return None

        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        result = rsi.iloc[-1] if not rsi.empty else None

        # RSIëŠ” 0-100 ë²”ìœ„
        return validate_numeric(result, min_val=0, max_val=100)
    except Exception as e:
        return None


def calculate_macd(prices, fast=12, slow=26, signal=9):
    """MACD ê³„ì‚°"""
    if len(prices) < slow + signal:
        return None, None, None

    ema_fast = prices.ewm(span=fast).mean()
    ema_slow = prices.ewm(span=slow).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal).mean()
    histogram = macd_line - signal_line

    return (
        macd_line.iloc[-1] if not macd_line.empty else None,
        signal_line.iloc[-1] if not signal_line.empty else None,
        histogram.iloc[-1] if not histogram.empty else None
    )


def calculate_bollinger_bands(prices, window=20, num_std=2):
    """ë³¼ë¦°ì €ë°´ë“œ ê³„ì‚°"""
    if len(prices) < window:
        return None, None, None

    sma = prices.rolling(window).mean()
    std = prices.rolling(window).std()
    upper = sma + (std * num_std)
    lower = sma - (std * num_std)

    current_price = prices.iloc[-1]
    bb_position = (current_price - lower.iloc[-1]) / (upper.iloc[-1] - lower.iloc[-1]) if upper.iloc[-1] != lower.iloc[
        -1] else None

    # BB Positionì€ 0-1 ë²”ìœ„ (ê·¹ë‹¨ì ì¸ ê²½ìš° -0.5 ~ 1.5 í—ˆìš©)
    bb_position = validate_numeric(bb_position, min_val=-0.5, max_val=1.5, allow_negative=True)

    return (
        upper.iloc[-1] if not upper.empty else None,
        lower.iloc[-1] if not lower.empty else None,
        bb_position
    )


def calculate_52week_high_low(prices):
    """52ì£¼ ê³ ê°€/ì €ê°€ ê³„ì‚°"""
    if len(prices) < 252:  # 1ë…„ ê±°ë˜ì¼
        high_52w = prices.max()
        low_52w = prices.min()
    else:
        high_52w = prices.tail(252).max()
        low_52w = prices.tail(252).min()

    current_price = prices.iloc[-1]
    high_ratio = current_price / high_52w if high_52w > 0 else None
    low_ratio = current_price / low_52w if low_52w > 0 else None

    # ë¹„ìœ¨ì€ 0-2 ë²”ìœ„ (í˜„ì¬ê°€ê°€ 52ì£¼ ìµœê³ ê°€ì˜ 2ë°°ê¹Œì§€ë§Œ í—ˆìš©)
    high_ratio = validate_numeric(high_ratio, min_val=0, max_val=2)
    low_ratio = validate_numeric(low_ratio, min_val=0, max_val=20)  # ì €ê°€ ëŒ€ë¹„ëŠ” ë” í° ë²”ìœ„

    return high_52w, low_52w, high_ratio, low_ratio


# ============== ê°•í™”ëœ PER ê³„ì‚° í•¨ìˆ˜ ==============

def calculate_pe_ratio(ticker, price, info, df_q, df_a):
    """ê°•í™”ëœ PER ê³„ì‚° (4ê°€ì§€ ë°©ë²• ì‹œë„) + ì´ìƒì¹˜ ì œê±°"""
    pe_values = []

    # ë°©ë²• 1: yfinance infoì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
    try:
        trailing_pe = info.get("trailingPE")
        forward_pe = info.get("forwardPE")
        if trailing_pe and trailing_pe > 0:
            validated_pe = validate_ratio(trailing_pe, min_ratio=0.1, max_ratio=500)
            if validated_pe:
                pe_values.append(validated_pe)
        if forward_pe and forward_pe > 0:
            validated_pe = validate_ratio(forward_pe, min_ratio=0.1, max_ratio=500)
            if validated_pe:
                pe_values.append(validated_pe)
    except Exception as e:
        pass

    # ë°©ë²• 2: trailing EPS ì‚¬ìš©
    try:
        trailing_eps = info.get("trailingEps")
        if trailing_eps and trailing_eps > 0 and price and price > 0:
            pe_calculated = price / trailing_eps
            if 0 < pe_calculated < 1000:
                pe_values.append(pe_calculated)
    except:
        pass

    # ë°©ë²• 3: ë¶„ê¸°ë³„ ë°ì´í„°ë¡œ TTM EPS ê³„ì‚°
    try:
        if df_q is not None and not df_q.empty and df_a is not None and not df_a.empty:
            eps_aliases = ["diluted eps", "basic eps", "eps (diluted)", "eps (basic)", "earnings per share", "eps"]
            eps_row = None
            for alias in eps_aliases:
                if alias in [str(x).lower() for x in df_q.index]:
                    eps_row = [x for x in df_q.index if str(x).lower() == alias][0]
                    break

            if eps_row is None:
                ni_aliases = ["net income", "net income common stockholders"]
                shares_aliases = ["diluted average shares", "weighted average shares diluted"]

                ni_row = None
                shares_row = None

                for alias in ni_aliases:
                    if alias in [str(x).lower() for x in df_q.index]:
                        ni_row = [x for x in df_q.index if str(x).lower() == alias][0]
                        break

                for alias in shares_aliases:
                    if alias in [str(x).lower() for x in df_a.index]:
                        shares_row = [x for x in df_a.index if str(x).lower() == alias][0]
                        break

                if ni_row and shares_row:
                    cols = sorted(df_q.columns, reverse=True)[:4]
                    ni_ttm = pd.to_numeric(df_q.loc[ni_row, cols], errors="coerce").sum()
                    shares = pd.to_numeric(df_a.loc[shares_row, cols[0]], errors="coerce")

                    if ni_ttm and shares and shares > 0:
                        eps_ttm = ni_ttm / shares
                        if eps_ttm > 0 and price > 0:
                            pe_calculated = price / eps_ttm
                            if 0 < pe_calculated < 1000:
                                pe_values.append(pe_calculated)
            else:
                cols = sorted(df_q.columns, reverse=True)[:4]
                eps_ttm = pd.to_numeric(df_q.loc[eps_row, cols], errors="coerce").sum()
                if eps_ttm and eps_ttm > 0 and price > 0:
                    pe_calculated = price / eps_ttm
                    if 0 < pe_calculated < 1000:
                        pe_values.append(pe_calculated)
    except Exception:
        pass

    # ë°©ë²• 4: ì—°ê°„ ë°ì´í„° ì‚¬ìš©
    try:
        if df_a is not None and not df_a.empty:
            eps_aliases = ["diluted eps", "basic eps", "eps (diluted)", "eps (basic)", "earnings per share", "eps"]
            eps_row = None
            for alias in eps_aliases:
                if alias in [str(x).lower() for x in df_a.index]:
                    eps_row = [x for x in df_a.index if str(x).lower() == alias][0]
                    break

            if eps_row:
                latest_col = sorted(df_a.columns, reverse=True)[0]
                eps_annual = pd.to_numeric(df_a.loc[eps_row, latest_col], errors="coerce")
                if eps_annual and eps_annual > 0 and price > 0:
                    pe_calculated = price / eps_annual
                    if 0 < pe_calculated < 1000:
                        pe_values.append(pe_calculated)
    except Exception:
        pass

    # ìœ íš¨í•œ PER ê°’ë“¤ ì¤‘ ì¤‘ê°„ê°’ ë°˜í™˜ (ì´ìƒì¹˜ ì œê±°)
    valid_pes = [pe for pe in pe_values if pe is not None and 0 < pe < 500]
    if valid_pes:
        median_pe = np.median(valid_pes)
        return validate_ratio(median_pe, min_ratio=0.1, max_ratio=500)

    return None


def _normalize_ticker(t):
    return str(t).strip().upper().replace(".", "-")


def _read_html(url: str):
    try:
        r = session.get(url, timeout=CONFIG["REQUEST_TIMEOUT"])
        r.raise_for_status()
        return pd.read_html(io.StringIO(r.text))
    except Exception as e:
        return []


def get_sp500_symbols():
    """S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    urls = [
        "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies",
        "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/main/data/constituents.csv"
    ]

    for url in urls:
        try:
            if "wikipedia" in url:
                tables = _read_html(url)
                if tables:
                    df = tables[0]
                    col = next((c for c in df.columns if str(c).lower().startswith("symbol")), "Symbol")
                    syms = df[col].dropna().astype(str).tolist()
                    print(f"[S&P500] Wikipediaì—ì„œ {len(syms)}ê°œ ì¢…ëª© ë¡œë“œ")
                    return [_normalize_ticker(s) for s in syms]
            else:
                r = session.get(url, timeout=CONFIG["REQUEST_TIMEOUT"])
                r.raise_for_status()
                df = pd.read_csv(io.StringIO(r.text))
                if 'Symbol' in df.columns:
                    syms = df['Symbol'].dropna().astype(str).tolist()
                    print(f"[S&P500] GitHubì—ì„œ {len(syms)}ê°œ ì¢…ëª© ë¡œë“œ")
                    return [_normalize_ticker(s) for s in syms]
        except Exception as e:
            continue

    fallback_sp500 = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'GOOG', 'TSLA', 'BRK-B', 'UNH', 'JNJ', 'XOM',
                      'JPM', 'V', 'NVDA', 'PG', 'MA', 'HD', 'CVX', 'LLY', 'ABBV', 'PFE']
    print(f"[S&P500] í´ë°±: {len(fallback_sp500)}ê°œ ì£¼ìš” ì¢…ëª© ì‚¬ìš©")
    return fallback_sp500


def _fetch_text(url):
    try:
        r = session.get(url, timeout=CONFIG["REQUEST_TIMEOUT"], allow_redirects=True)
        r.raise_for_status()
        return r.text
    except Exception as e:
        return ""


def _read_pipe_text_to_df(text: str) -> pd.DataFrame:
    try:
        return pd.read_csv(io.StringIO(text), sep="|")
    except Exception:
        return pd.DataFrame()


def _normalize_symbol_df(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    cols = {c.lower(): c for c in df.columns}
    sym = cols.get("symbol") or cols.get("act symbol") or cols.get("nasdaq symbol") or list(df.columns)[0]

    out = df.copy()
    out.rename(columns={sym: "Symbol"}, inplace=True)
    out["Symbol"] = out["Symbol"].astype(str).str.upper().str.replace(".", "-", regex=False)

    if "TestIssue" in out.columns:
        mask_test = out["TestIssue"].astype(str).str.upper().ne("Y")
        out = out[mask_test]

    if "FinancialStatus" in out.columns:
        fin_s = out["FinancialStatus"].astype(str).str.upper()
        mask_fin = (~fin_s.isin(["D", "E", "H", "S", "C", "T"]))
        out = out[mask_fin]

    return out


def _filter_common_stock(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    name_str = df.get("SecurityName", pd.Series([""] * len(df))).astype(str).str.lower()

    is_common_kw = name_str.str.contains(
        r"common stock|ordinary shares|class [ab]\s+common|shs",
        regex=True, na=False
    )
    is_deriv_kw = name_str.str.contains(
        r"warrant|right|unit|preferred|preference|pref|etf|fund|trust|note|debenture|bond|adr|adr\.",
        regex=True, na=False
    )

    base = df[is_common_kw & ~is_deriv_kw]
    return base if not base.empty else df[~is_deriv_kw]


def get_all_us_listed_common():
    """ëª¨ë“  ë¯¸êµ­ ìƒì¥ ì£¼ì‹ ì¢…ëª© ê°€ì ¸ì˜¤ê¸°"""
    urls = [
        "https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt",
        "https://www.nasdaqtrader.com/dynamic/SymDir/otherlisted.txt",
        "https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download",
        "https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download",
    ]

    dfs = []

    for u in urls:
        try:
            txt = _fetch_text(u)
            if not txt:
                continue

            df = _normalize_symbol_df(_read_pipe_text_to_df(txt))
            if not df.empty:
                dfs.append(df)
                print(f"[US_ALL] {u}ì—ì„œ {len(df)}ê°œ ì¢…ëª© ë¡œë“œ")

        except Exception:
            continue

    if not dfs:
        print("[US_ALL] ëª¨ë“  ì†ŒìŠ¤ ì‹¤íŒ¨, í´ë°± ì¢…ëª© ì‚¬ìš©")
        fallback_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'BRK-B', 'V', 'JNJ', 'WMT',
                            'PG', 'JPM', 'UNH', 'HD', 'DIS', 'PYPL', 'NFLX', 'ADBE', 'CRM', 'INTC']
        return fallback_tickers

    df_combined = pd.concat(dfs, ignore_index=True)
    df_combined = _filter_common_stock(df_combined)

    syms = df_combined["Symbol"].dropna().unique().tolist()
    print(f"[US_ALL] í•„í„° í›„ ì´ {len(syms)}ê°œ ì¢…ëª©")
    return sorted(syms)


def load_universe():
    """ìœ ë‹ˆë²„ìŠ¤ ë¡œë“œ"""
    src = CONFIG["UNIVERSE_SOURCE"]

    try:
        if src == "sp500":
            u = get_sp500_symbols()
        elif src == "us_all":
            u = get_all_us_listed_common()
        elif src == "custom":
            u = [_normalize_ticker(x) for x in CONFIG["CUSTOM_TICKERS"]]
        else:
            raise ValueError("UNIVERSE_SOURCEëŠ” us_all, sp500, custom ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")

        if CONFIG["SHUFFLE_UNIVERSE"]:
            random.shuffle(u)

        if CONFIG["MAX_TICKERS"]:
            u = u[CONFIG["UNIVERSE_OFFSET"]:CONFIG["UNIVERSE_OFFSET"] + CONFIG["MAX_TICKERS"]]
        elif CONFIG["UNIVERSE_OFFSET"]:
            u = u[CONFIG["UNIVERSE_OFFSET"]:]

        print(f"[ìœ ë‹ˆë²„ìŠ¤] {src} ì´={len(u)}ê°œ ìƒ˜í”Œ={u[:8]}")
        return u

    except Exception as e:
        print(f"[ìœ ë‹ˆë²„ìŠ¤] ë¡œë“œ ì‹¤íŒ¨: {e}")
        return ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'BRK-B', 'JNJ', 'JPM', 'V']


# ============== OHLCV â†’ ë¼ì´íŠ¸ ì§€í‘œ ==============

def _compute_enhanced_ta_single(c, h, l, v):
    """ê°œì„ ëœ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° + ì´ìƒì¹˜ ê²€ì¦"""
    try:
        # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ ì™„í™”: 50ê°œ -> 20ê°œ (ë” ë§ì€ ì¢…ëª© ìˆ˜ì§‘ ê°€ëŠ¥)
        if c is None or len(c.dropna()) < 20:
            return None

        c_clean = c.dropna()
        if len(c_clean) == 0:
            return None

        last_close = float(c_clean.iloc[-1])
        last_close = validate_price(last_close)
        if last_close is None:
            return None

        # ê¸°ë³¸ ì§€í‘œë“¤
        s20 = c_clean.rolling(20).mean().iloc[-1] if len(c_clean) >= 20 else None
        s50 = c_clean.rolling(50).mean().iloc[-1] if len(c_clean) >= 50 else None
        s200 = c_clean.rolling(200).mean().iloc[-1] if len(c_clean) >= 200 else None

        ret5 = c_clean.pct_change(5).iloc[-1] if len(c_clean) >= 6 else None
        ret20 = c_clean.pct_change(20).iloc[-1] if len(c_clean) >= 21 else None
        ret63 = c_clean.pct_change(63).iloc[-1] if len(c_clean) >= 64 else None

        # ìˆ˜ìµë¥  ê²€ì¦
        ret5 = validate_percentage(ret5, min_pct=-0.99, max_pct=9.99)
        ret20 = validate_percentage(ret20, min_pct=-0.99, max_pct=9.99)
        ret63 = validate_percentage(ret63, min_pct=-0.99, max_pct=9.99)

        # ê±°ë˜ëŸ‰ ì§€í‘œ
        avg20_vol = today_vol = rvol = None
        if v is not None and len(v.dropna()) > 0:
            v_clean = v.dropna()
            avg20_vol = float(v_clean.rolling(20).mean().iloc[-1]) if len(v_clean) >= 20 else float(v_clean.mean())
            today_vol = float(v_clean.iloc[-1]) if len(v_clean) > 0 else None

            avg20_vol = validate_volume(avg20_vol)
            today_vol = validate_volume(today_vol)

            if avg20_vol and today_vol and avg20_vol > 0:
                rvol = today_vol / avg20_vol
                rvol = validate_numeric(rvol, min_val=0, max_val=100)

        # ATR
        atr = atr_pct = None
        if h is not None and l is not None and len(h.dropna()) > 0 and len(l.dropna()) > 0:
            h_clean, l_clean = h.dropna(), l.dropna()
            if len(h_clean) >= 14 and len(l_clean) >= 14:
                prev_close = c_clean.shift(1)
                tr = pd.concat([
                    h_clean - l_clean,
                    (h_clean - prev_close).abs(),
                    (l_clean - prev_close).abs()
                ], axis=1).max(axis=1)
                atr = float(tr.rolling(14).mean().iloc[-1]) if len(tr) >= 14 else None
                if atr and last_close > 0:
                    atr_pct = atr / last_close
                    atr_pct = validate_percentage(atr_pct, min_pct=0, max_pct=1.0)

        # ì‹ ê·œ ê¸°ìˆ ì  ì§€í‘œë“¤
        rsi_14 = calculate_rsi(c_clean, 14)
        macd, macd_signal, macd_histogram = calculate_macd(c_clean)
        bb_upper, bb_lower, bb_position = calculate_bollinger_bands(c_clean)
        high_52w, low_52w, high_52w_ratio, low_52w_ratio = calculate_52week_high_low(c_clean)

        # ëª¨ë©˜í…€ ì§€í‘œ
        momentum_12m = None
        if len(c_clean) >= 252:
            momentum_12m = (last_close / c_clean.iloc[-252]) - 1
            momentum_12m = validate_percentage(momentum_12m, min_pct=-0.99, max_pct=9.99)

        volatility_21d = None
        if len(c_clean) >= 22:
            volatility_21d = c_clean.pct_change().rolling(21).std().iloc[-1]
            volatility_21d = validate_percentage(volatility_21d, min_pct=0, max_pct=1.0)

        return {
            # ê¸°ë³¸ ì§€í‘œ
            "last_price": last_close,
            "sma20": float(s20) if s20 else None,
            "sma50": float(s50) if s50 else None,
            "sma200": float(s200) if s200 else None,
            "ret5": float(ret5) if ret5 else None,
            "ret20": float(ret20) if ret20 else None,
            "ret63": float(ret63) if ret63 else None,
            "avg20_vol": avg20_vol,
            "today_vol": today_vol,
            "rvol": rvol,
            "atr": atr,
            "atr_pct": atr_pct,

            # ì‹ ê·œ ê¸°ìˆ ì  ì§€í‘œ
            "rsi_14": rsi_14,
            "macd": macd,
            "macd_signal": macd_signal,
            "macd_histogram": macd_histogram,
            "bb_upper": bb_upper,
            "bb_lower": bb_lower,
            "bb_position": bb_position,
            "high_52w": high_52w,
            "low_52w": low_52w,
            "high_52w_ratio": high_52w_ratio,
            "low_52w_ratio": low_52w_ratio,
            "momentum_12m": momentum_12m,
            "volatility_21d": volatility_21d,
        }
    except Exception:
        return None


def _compute_ta_metrics(df):
    """DataFrameì—ì„œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    out = {}

    try:
        if isinstance(df.columns, pd.MultiIndex):
            fields = set(df.columns.get_level_values(0))
            tickers = sorted(set(df.columns.get_level_values(1)))

            close_col = "Adj Close" if "Adj Close" in fields else "Close"

            for t in tickers:
                try:
                    if (close_col, t) not in df.columns:
                        continue

                    c = df[(close_col, t)].dropna()
                    h = df[("High", t)].dropna() if ("High", t) in df.columns else None
                    l = df[("Low", t)].dropna() if ("Low", t) in df.columns else None
                    v = df[("Volume", t)].dropna() if ("Volume", t) in df.columns else None

                    metrics = _compute_enhanced_ta_single(c, h, l, v)
                    if metrics:
                        out[t] = metrics
                except Exception:
                    continue
        else:
            close_col = "Adj Close" if "Adj Close" in df.columns else "Close"
            c = df[close_col] if close_col in df.columns else None
            h = df["High"] if "High" in df.columns else None
            l = df["Low"] if "Low" in df.columns else None
            v = df["Volume"] if "Volume" in df.columns else None

            metrics = _compute_enhanced_ta_single(c, h, l, v)
            if metrics:
                out["__SINGLE__"] = metrics

    except Exception:
        pass

    return out


def safe_yf_download(tickers, **kwargs):
    """ì•ˆì „í•œ yfinance ë‹¤ìš´ë¡œë“œ with ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬"""
    max_retries = kwargs.pop('max_retries', 3)
    ticker_str = tickers if isinstance(tickers, str) else f"batch({len(tickers)})"

    for attempt in range(max_retries):
        try:
            data = yf.download(tickers, **kwargs)
            if data is not None and not data.empty:
                return data
            elif attempt == max_retries - 1:
                log_error("YF_DOWNLOAD", ticker_str, "Empty data returned")
        except Exception as e:
            if attempt < max_retries - 1:
                sleep_time = (2 ** attempt) + random.uniform(0, 1)
                log_error("YF_DOWNLOAD", ticker_str, f"Attempt {attempt+1} failed: {str(e)}, retrying in {sleep_time:.1f}s")
                time.sleep(sleep_time)
            else:
                log_error("YF_DOWNLOAD", ticker_str, f"All {max_retries} attempts failed: {str(e)}")

    return None


# â­â­â­ OHLCV ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
def process_ohlcv_batch(args):
    """ë‹¨ì¼ ë°°ì¹˜ OHLCV ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    batch, batch_idx, total_batches, period = args

    TA_batch = {}
    PX_batch = {}
    VOL_batch = {}
    ok_tickers_batch = set()

    # ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹œë„
    batch_data = None
    for attempt in range(CONFIG["BATCH_RETRIES"]):
        try:
            batch_data = safe_yf_download(
                batch,
                period=period,
                interval="1d",
                auto_adjust=False,
                progress=False,
                threads=False,
                timeout=30
            )
            if batch_data is not None and not batch_data.empty:
                break
        except Exception:
            if attempt < CONFIG["BATCH_RETRIES"] - 1:
                time.sleep((1.5 ** attempt) + random.random())

    processed_count = 0

    # ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
    if batch_data is not None and not batch_data.empty:
        metrics = _compute_ta_metrics(batch_data)

        if isinstance(batch_data.columns, pd.MultiIndex):
            close_col = "Adj Close" if "Adj Close" in set(batch_data.columns.get_level_values(0)) else "Close"

            for t in batch:
                try:
                    if (close_col, t) not in batch_data.columns:
                        continue

                    prices = batch_data[(close_col, t)].dropna()
                    if len(prices) < 5:
                        continue

                    last_price = float(prices.iloc[-1])
                    last_price = validate_price(last_price)
                    if last_price is None:
                        continue

                    avg_vol = 0
                    if ("Volume", t) in batch_data.columns:
                        vols = batch_data[("Volume", t)].dropna()
                        avg_vol = float(vols.rolling(20).mean().iloc[-1]) if len(vols) >= 20 else float(vols.mean())
                        avg_vol = validate_volume(avg_vol) or 0

                    ok_tickers_batch.add(t)
                    PX_batch[t] = last_price
                    VOL_batch[t] = avg_vol

                    if t in metrics:
                        TA_batch[t] = metrics[t]
                    else:
                        TA_batch[t] = {
                            "last_price": last_price,
                            "sma20": last_price,
                            "sma50": last_price,
                            "ret5": 0.0,
                            "ret20": 0.0,
                            "avg20_vol": avg_vol,
                            "rvol": 1.0,
                            "atr_pct": 0.02
                        }

                    processed_count += 1

                except Exception:
                    continue
        else:
            if batch and len(batch) == 1:
                t = batch[0]
                try:
                    close_col = "Adj Close" if "Adj Close" in batch_data.columns else "Close"
                    prices = batch_data[close_col].dropna() if close_col in batch_data.columns else None
                    if prices is not None and len(prices) >= 5:
                        last_price = float(prices.iloc[-1])
                        last_price = validate_price(last_price)
                        if last_price is not None:
                            avg_vol = 0
                            if "Volume" in batch_data.columns:
                                vols = batch_data["Volume"].dropna()
                                avg_vol = float(vols.rolling(20).mean().iloc[-1]) if len(vols) >= 20 else float(
                                    vols.mean())
                                avg_vol = validate_volume(avg_vol) or 0

                            ok_tickers_batch.add(t)
                            PX_batch[t] = last_price
                            VOL_batch[t] = avg_vol

                            if "__SINGLE__" in metrics:
                                TA_batch[t] = metrics["__SINGLE__"]
                            else:
                                TA_batch[t] = {
                                    "last_price": last_price,
                                    "sma20": last_price,
                                    "sma50": last_price,
                                    "ret5": 0.0,
                                    "ret20": 0.0,
                                    "avg20_vol": avg_vol,
                                    "rvol": 1.0,
                                    "atr_pct": 0.02
                                }
                            processed_count += 1
                except Exception:
                    pass

    # ë°°ì¹˜ ì™„ì „ ì‹¤íŒ¨ ì‹œ ì „ì²´ ê°œë³„ ë‹¤ìš´ë¡œë“œ, ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ëˆ„ë½ëœ ê²ƒë§Œ ê°œë³„ ë‹¤ìš´ë¡œë“œ
    if processed_count == 0:
        # ë°°ì¹˜ ì „ì²´ ì‹¤íŒ¨ - ëª¨ë“  í‹°ì»¤ ê°œë³„ ë‹¤ìš´ë¡œë“œ
        retry_tickers = batch
    elif processed_count < len(batch):
        # ë¶€ë¶„ ì„±ê³µ - ì‹¤íŒ¨í•œ í‹°ì»¤ë§Œ ê°œë³„ ë‹¤ìš´ë¡œë“œ
        retry_tickers = [t for t in batch if t not in ok_tickers_batch]
        if CONFIG.get("VERBOSE_LOGGING", False):
            print(f"  [ë°°ì¹˜ {batch_idx}] ë¶€ë¶„ ì„±ê³µ: {processed_count}/{len(batch)}, ëˆ„ë½ {len(retry_tickers)}ê°œ ì¬ì‹œë„")
    else:
        # ì „ì²´ ì„±ê³µ
        retry_tickers = []

    # ê°œë³„ ë‹¤ìš´ë¡œë“œ
    if retry_tickers:
        for t in retry_tickers:
            for attempt in range(CONFIG["SINGLE_RETRIES"]):
                try:
                    data = safe_yf_download(
                        t,
                        period=period,
                        interval="1d",
                        auto_adjust=False,
                        progress=False,
                        threads=False,
                        timeout=30,
                        max_retries=2
                    )
                    if data is not None and not data.empty:
                        metrics = _compute_ta_metrics(data)
                        if "__SINGLE__" in metrics:
                            close_col = "Adj Close" if "Adj Close" in data.columns else "Close"
                            prices = data[close_col].dropna()

                            if len(prices) >= 5:
                                last_price = float(prices.iloc[-1])
                                last_price = validate_price(last_price)
                                if last_price is not None:
                                    avg_vol = 0
                                    if "Volume" in data.columns:
                                        vols = data["Volume"].dropna()
                                        avg_vol = float(vols.rolling(20).mean().iloc[-1]) if len(vols) >= 20 else float(
                                            vols.mean())
                                        avg_vol = validate_volume(avg_vol) or 0

                                    ok_tickers_batch.add(t)
                                    PX_batch[t] = last_price
                                    VOL_batch[t] = avg_vol
                                    TA_batch[t] = metrics["__SINGLE__"]
                                    processed_count += 1
                        break
                except Exception:
                    if attempt < CONFIG["SINGLE_RETRIES"] - 1:
                        time.sleep((1.5 ** attempt) + random.random() * 0.3)

    return batch_idx, TA_batch, PX_batch, VOL_batch, ok_tickers_batch, processed_count, len(batch)


def preload_ohlcv_light(tickers, period="120d", chunk=50, **kwargs):
    """â­ ë³‘ë ¬ ì²˜ë¦¬ëœ OHLCV ë°ì´í„° í”„ë¦¬ë¡œë“œ"""
    TA, PX, VOL = {}, {}, {}
    ok_tickers = set()

    print(f"[OHLCV] {len(tickers)}ê°œ ì¢…ëª© ë¡œë“œ ì‹œì‘...")
    print(f"[OHLCV] {CONFIG['OHLCV_WORKERS']}ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì²˜ë¦¬...")

    # ë°°ì¹˜ ìƒì„±
    batches = []
    total_batches = (len(tickers) + chunk - 1) // chunk
    for i in range(0, len(tickers), chunk):
        batch = tickers[i:i + chunk]
        batch_idx = i // chunk + 1
        batches.append((batch, batch_idx, total_batches, period))

    # ë³‘ë ¬ ì²˜ë¦¬
    total_processed = 0
    completed = 0
    with ThreadPoolExecutor(max_workers=CONFIG["OHLCV_WORKERS"]) as executor:
        futures = {executor.submit(process_ohlcv_batch, batch_info): batch_info for batch_info in batches}

        for future in as_completed(futures):
            try:
                batch_idx, TA_batch, PX_batch, VOL_batch, ok_batch, processed, total = future.result()

                # ê²°ê³¼ ë³‘í•©
                TA.update(TA_batch)
                PX.update(PX_batch)
                VOL.update(VOL_batch)
                ok_tickers.update(ok_batch)
                total_processed += processed
                completed += 1

                if completed % 10 == 0 or completed == total_batches:
                    print(f"[OHLCV] ì§„í–‰: {completed}/{total_batches} ë°°ì¹˜ ì™„ë£Œ (ëˆ„ì : {total_processed}/{len(tickers)} ì¢…ëª©)")

            except Exception:
                continue

    print(f"[OHLCV] ì „ì²´ ì™„ë£Œ: {len(ok_tickers)}/{len(tickers)}ê°œ ì¢…ëª© ì„±ê³µ")
    return TA, PX, VOL, ok_tickers


# ============== ìƒì„¸ ì¬ë¬´ ìœ í‹¸ ==============
REV_ALIASES = ["total revenue", "revenues", "revenue", "net sales", "sales", "total net sales"]
OP_ALIASES = ["operating income", "operating income (loss)", "income from operations", "operating profit",
              "operating profit (loss)", "ebit"]
FCF_ALIASES = ["free cash flow", "free cashflow", "freecashflow"]
DA_ALIASES = ["depreciation", "depreciation & amortization", "depreciation and amortization"]
EPS_ALIASES = ["diluted eps", "basic eps", "eps (diluted)", "eps (basic)", "earnings per share", "eps"]
NET_INCOME_ALIASES = ["net income", "net income common stockholders", "net income applicable to common shares"]
DIL_SHARES_ALIASES = ["diluted average shares", "weighted average shares diluted",
                      "weighted average diluted shares outstanding", "weighted average diluted shares",
                      "weighted average shares - diluted", "weighted average number of shares diluted"]


def _find_row(index_like, aliases, exclude=None):
    if index_like is None: return None
    exclude = [w.lower() for w in (exclude or [])]
    idx = [str(x).lower() for x in index_like]
    for key in aliases:
        k = key.lower()
        for i, s in enumerate(idx):
            if k in s and not any(x in s for x in exclude):
                return index_like[i]
    return None


def coalesce(*vals):
    for v in vals:
        try:
            if v is None: continue
            if isinstance(v, float) and math.isnan(v): continue
            return v
        except Exception:
            continue
    return None


def ttm_sum(df: pd.DataFrame, row, n=4, absolute=False):
    if df is None or df.empty or row not in df.index or df.shape[1] < n: return None
    cols = sorted(df.columns, reverse=True)[:n]
    try:
        vals = pd.to_numeric(df.loc[row, cols], errors="coerce").fillna(0)
        result = float(vals.abs().sum()) if absolute else float(vals.sum())
        return result if not math.isnan(result) else None
    except Exception:
        return None


def ttm_yoy_growth(df_q: pd.DataFrame, row):
    if df_q is None or df_q.empty or row not in df_q.index or df_q.shape[1] < 8: return None
    cols = sorted(df_q.columns, reverse=True)
    try:
        curr = float(pd.to_numeric(df_q.loc[row, cols[:4]], errors="coerce").fillna(0).sum())
        prev = float(pd.to_numeric(df_q.loc[row, cols[4:8]], errors="coerce").fillna(0).sum())
    except Exception:
        return None
    if prev <= 0: return None
    growth = (curr / prev) - 1.0
    return validate_percentage(growth, min_pct=-0.99, max_pct=9.99)


def annual_yoy_growth(df_a: pd.DataFrame, row):
    if df_a is None or df_a.empty or row not in df_a.index or df_a.shape[1] < 2: return None
    cols = sorted(df_a.columns, reverse=True)[:2]
    try:
        curr = float(pd.to_numeric(df_a.loc[row, cols[0]], errors="coerce"))
        prev = float(pd.to_numeric(df_a.loc[row, cols[1]], errors="coerce"))
    except Exception:
        return None
    if prev <= 0: return None
    growth = (curr / prev) - 1.0
    return validate_percentage(growth, min_pct=-0.99, max_pct=9.99)


def _last4_sum_row(df, aliases):
    if df is None or df.empty: return None
    row = _find_row(df.index, aliases)
    if not row or df.shape[1] < 4: return None
    cols = sorted(df.columns, reverse=True)[:4]
    return float(pd.to_numeric(df.loc[row, cols], errors="coerce").fillna(0).sum())


def _last_col(df, aliases):
    if df is None or df.empty: return None
    row = _find_row(df.index, aliases)
    if not row: return None
    col = sorted(df.columns, reverse=True)[0]
    return float(pd.to_numeric(df.loc[row, col], errors="coerce"))


def _eps_ttm_from_statements(df_q, df_a):
    ni = _last4_sum_row(df_q, NET_INCOME_ALIASES)
    sh = _last_col(df_a, DIL_SHARES_ALIASES)
    if ni and sh and sh > 0: return ni / sh
    return None


def _safe_df(getter, max_retries=2):
    """DataFrame ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° with ì¬ì‹œë„"""
    for attempt in range(max_retries):
        try:
            df = getter()
            if df is not None and hasattr(df, 'empty') and not df.empty:
                return df
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(0.5 + random.uniform(0, 0.5))
            # ë§ˆì§€ë§‰ ì‹œë„ ì‹¤íŒ¨ ì‹œëŠ” ì¡°ìš©íˆ ì‹¤íŒ¨ (ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€)
    return None


def _parse_growth_to_pct(val):
    """ì„±ì¥ë¥  íŒŒì‹±"""
    if val is None: return None
    try:
        if isinstance(val, str):
            s = val.strip().replace('%', '').replace('+', '')
            if s.lower() in {'n/a', 'na', 'nan', 'none', '-', ''}: return None
            return float(s)
        x = float(val)
        return x * 100.0 if abs(x) <= 1.0 else x
    except Exception:
        return None


def get_eps_annual_series(tic: yf.Ticker):
    """EPS ì—°ê°„ ì‹œë¦¬ì¦ˆ ê°€ì ¸ì˜¤ê¸°"""
    df_a = None
    try:
        df_a = tic.income_stmt
        if df_a is None or df_a.empty: df_a = tic.financials
    except Exception:
        pass
    if df_a is not None and not df_a.empty:
        row_eps = _find_row(df_a.index, EPS_ALIASES)
        if row_eps:
            try:
                vals = pd.to_numeric(df_a.loc[row_eps], errors="coerce").dropna()
                return list(vals.sort_index().values)
            except Exception:
                pass
        else:
            ni_row = _find_row(df_a.index, NET_INCOME_ALIASES)
            sh_row = _find_row(df_a.index, DIL_SHARES_ALIASES)
            if ni_row and sh_row:
                try:
                    ni = pd.to_numeric(df_a.loc[ni_row], errors="coerce")
                    sh = pd.to_numeric(df_a.loc[sh_row], errors="coerce").replace(0, np.nan)
                    vals = (ni / sh).dropna()
                    return list(vals.sort_index().values)
                except Exception:
                    pass
    try:
        earn = tic.earnings
        if earn is not None and not earn.empty:
            info = {}
            try:
                info = tic.get_info() or {}
            except Exception:
                pass
            so = info.get("sharesOutstanding")
            if so and so > 0:
                ser = pd.to_numeric(earn["Earnings"], errors="coerce") / float(so)
                return list(ser.sort_index().dropna().values)
    except Exception:
        pass
    return []


def eps_cagr_from_series(vals, min_years=3, max_years=5):
    """EPS CAGR ê³„ì‚°"""
    v = [float(x) for x in vals if x is not None and not np.isnan(x)]
    if len(v) < min_years: return None
    use = v[-max_years:]
    if len(use) < min_years: return None
    first, last = use[0], use[-1]
    if first <= 0 or last <= 0: return None
    years = len(use) - 1
    if years <= 0: return None
    cagr = (last / first) ** (1.0 / years) - 1.0
    return validate_percentage(cagr, min_pct=-0.99, max_pct=9.99)


def calculate_missing_financials(ticker, info, df_q, df_a, cf_q, balance_a, price):
    """ëˆ„ë½ëœ ì¬ë¬´ ë°ì´í„° ê³„ì‚° + ì´ìƒì¹˜ ê²€ì¦"""
    calculated = {}

    try:
        # 1. RevYoY ê³„ì‚°
        if calculated.get('RevYoY') is None and df_q is not None:
            rev_row = _find_row(df_q.index, REV_ALIASES, exclude=["per share", "operating revenue", "royalty"])
            if rev_row:
                rev_yoy = ttm_yoy_growth(df_q, rev_row)
                if rev_yoy is not None:
                    calculated['RevYoY'] = rev_yoy
                elif df_a is not None and rev_row in df_a.index:
                    rev_yoy = annual_yoy_growth(df_a, rev_row)
                    if rev_yoy is not None:
                        calculated['RevYoY'] = rev_yoy

        # 2. OpMarginTTM ê³„ì‚°
        if calculated.get('OpMarginTTM') is None and df_q is not None:
            rev_row = _find_row(df_q.index, REV_ALIASES, exclude=["per share", "operating revenue", "royalty"])
            op_row = _find_row(df_q.index, OP_ALIASES)
            if rev_row and op_row:
                rev_ttm = ttm_sum(df_q, rev_row, 4)
                op_ttm = ttm_sum(df_q, op_row, 4)
                if rev_ttm and op_ttm and rev_ttm > 0:
                    margin = op_ttm / rev_ttm
                    calculated['OpMarginTTM'] = validate_percentage(margin, min_pct=-1.0, max_pct=1.0)

        # 3. ROE ê³„ì‚°
        if calculated.get('ROE(info)') is None and df_a is not None and balance_a is not None:
            ni_row = _find_row(df_a.index, NET_INCOME_ALIASES)
            equity_row = _find_row(balance_a.index, ["total equity", "stockholders equity", "shareholders equity"])
            if ni_row and equity_row:
                ni = _last_col(df_a, [ni_row])
                equity = _last_col(balance_a, [equity_row])
                if ni and equity and equity > 0:
                    roe = ni / equity
                    calculated['ROE(info)'] = validate_percentage(roe, min_pct=-5.0, max_pct=5.0)

        # 4. EV/EBITDA ê³„ì‚°
        if calculated.get('EV_EBITDA') is None:
            ev = info.get("enterpriseValue")
            ebitda = info.get("ebitda")
            if ev and ebitda and ebitda > 0:
                ev_ebitda = ev / ebitda
                calculated['EV_EBITDA'] = validate_ratio(ev_ebitda, min_ratio=-100, max_ratio=500)

        # 5. FCF Yield ê³„ì‚°
        if calculated.get('FCF_Yield') is None and cf_q is not None:
            fcf_row = _find_row(cf_q.index, FCF_ALIASES)
            if fcf_row:
                fcf_ttm = ttm_sum(cf_q, fcf_row, 4)
                mktcap = info.get("marketCap")
                if fcf_ttm and mktcap and mktcap > 0:
                    fcf_yield = fcf_ttm / mktcap
                    calculated['FCF_Yield'] = validate_percentage(fcf_yield, min_pct=-1.0, max_pct=1.0)

        # 6. PB ê³„ì‚°
        if calculated.get('PB') is None and balance_a is not None:
            equity_row = _find_row(balance_a.index, ["total equity", "stockholders equity", "shareholders equity"])
            if equity_row and price:
                equity = _last_col(balance_a, [equity_row])
                shares = info.get("sharesOutstanding")
                if equity and shares and shares > 0:
                    bps = equity / shares
                    if bps > 0:
                        pb = price / bps
                        calculated['PB'] = validate_ratio(pb, min_ratio=0, max_ratio=100)

        # 7. PayoutRatio ê³„ì‚°
        if calculated.get('PayoutRatio') is None and df_a is not None:
            div_row = _find_row(df_a.index, ["dividends paid", "cash dividends paid", "dividend"])
            ni_row = _find_row(df_a.index, NET_INCOME_ALIASES)
            if div_row and ni_row:
                div_paid = _last_col(df_a, [div_row])
                ni = _last_col(df_a, [ni_row])
                if div_paid and ni and ni > 0:
                    payout = abs(div_paid) / ni
                    calculated['PayoutRatio'] = validate_percentage(payout, min_pct=0, max_pct=2.0)

    except Exception:
        pass

    return calculated


def _calculate_financial_ratios(q_is, a_is):
    """ì¬ë¬´ ë¹„ìœ¨ ê³„ì‚°"""
    rev_yoy = op_margin = None

    if q_is is not None and not q_is.empty:
        rev_row = _find_row(q_is.index, REV_ALIASES, exclude=["per share", "operating revenue", "royalty"])
        op_row = _find_row(q_is.index, OP_ALIASES)

        if rev_row:
            rev_ttm = ttm_sum(q_is, rev_row, 4)
            rev_yoy = ttm_yoy_growth(q_is, rev_row)

            if rev_yoy is None and a_is is not None and not a_is.empty and rev_row in a_is.index:
                rev_yoy = annual_yoy_growth(a_is, rev_row)

            if op_row and rev_ttm and rev_ttm > 0:
                op_ttm = ttm_sum(q_is, op_row, 4)
                if op_ttm:
                    op_margin = op_ttm / rev_ttm
                    op_margin = validate_percentage(op_margin, min_pct=-1.0, max_pct=1.0)

    return rev_yoy, op_margin


def _calculate_ev_ebitda(info, q_is):
    """EV/EBITDA ê³„ì‚°"""
    ev = info.get("enterpriseValue")
    ebitda = info.get("ebitda")
    ev_ebitda = None

    try:
        if ev and ebitda and float(ebitda) > 0:
            ev_ebitda = float(ev) / float(ebitda)
            ev_ebitda = validate_ratio(ev_ebitda, min_ratio=-100, max_ratio=500)
    except (TypeError, ValueError):
        pass

    return ev_ebitda


def _calculate_fcf_yield(info, cf_q):
    """FCF Yield ê³„ì‚°"""
    fcf_yield = None
    if cf_q is not None and not cf_q.empty:
        fcf_row = _find_row(cf_q.index, FCF_ALIASES)
        if fcf_row:
            fcf_ttm = ttm_sum(cf_q, fcf_row, 4)
            mktcap = info.get("marketCap")
            if fcf_ttm and mktcap and float(mktcap) > 0:
                fcf_yield = float(fcf_ttm) / float(mktcap)
                fcf_yield = validate_percentage(fcf_yield, min_pct=-1.0, max_pct=1.0)

    return fcf_yield


def _calculate_growth_indicators(q_is, a_is, info):
    """ì„±ì¥ì„± ì§€í‘œ ê³„ì‚°"""
    growth = {
        "EPS_Growth_3Y": None,
        "Revenue_Growth_3Y": None,
        "EBITDA_Growth_3Y": None,
    }

    try:
        # EPS ì„±ì¥ë¥ 
        eps_series = []
        if a_is is not None and not a_is.empty:
            eps_row = _find_row(a_is.index, EPS_ALIASES)
            if eps_row:
                eps_data = pd.to_numeric(a_is.loc[eps_row], errors="coerce").dropna()
                if len(eps_data) >= 3:
                    eps_series = list(eps_data.sort_index().values[-3:])

        if len(eps_series) >= 3:
            cagr = (eps_series[-1] / eps_series[0]) ** (1 / 2) - 1
            growth["EPS_Growth_3Y"] = validate_percentage(cagr, min_pct=-0.99, max_pct=9.99)

        # ë§¤ì¶œ ì„±ì¥ë¥ 
        if a_is is not None and not a_is.empty:
            rev_row = _find_row(a_is.index, REV_ALIASES)
            if rev_row:
                rev_data = pd.to_numeric(a_is.loc[rev_row], errors="coerce").dropna()
                if len(rev_data) >= 3:
                    rev_series = list(rev_data.sort_index().values[-3:])
                    if len(rev_series) >= 3:
                        cagr = (rev_series[-1] / rev_series[0]) ** (1 / 2) - 1
                        growth["Revenue_Growth_3Y"] = validate_percentage(cagr, min_pct=-0.99, max_pct=9.99)

    except Exception:
        pass

    return growth


# â­â­â­ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
def fetch_single_ticker_wrapper(args):
    """ë‹¨ì¼ í‹°ì»¤ ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    t, row = args
    try:
        rec = fetch_enhanced_details_for_ticker(
            t,
            price=row["Price"],
            avg_vol=(row["DollarVol($M)"] * 1_000_000) / max(1e-9, row["Price"])
        )

        # ë¼ì´íŠ¸ í•„ë“œ ë³‘í•©
        rec.update({
            "SMA20": row.get("SMA20"),
            "SMA50": row.get("SMA50"),
            "SMA200": row.get("SMA200"),
            "ATR_PCT": row.get("ATR_PCT"),
            "RVOL": row.get("RVOL"),
            "RET5": row.get("RET5"),
            "RET20": row.get("RET20"),
            "RET63": row.get("RET63"),
            "RSI_14": row.get("RSI_14"),
            "MACD": row.get("MACD"),
            "MACD_Signal": row.get("MACD_Signal"),
            "MACD_Histogram": row.get("MACD_Histogram"),
            "BB_Position": row.get("BB_Position"),
            "High_52W_Ratio": row.get("High_52W_Ratio"),
            "Low_52W_Ratio": row.get("Low_52W_Ratio"),
            "Momentum_12M": row.get("Momentum_12M"),
            "Volatility_21D": row.get("Volatility_21D"),
        })

        return rec, None
    except Exception as e:
        return None, f"ì¢…ëª© {t} ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"


def fetch_enhanced_details_for_ticker(tkr, price, avg_vol):
    """ê°œì„ ëœ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ with ì¬ì‹œë„"""
    t = None
    info = {}

    # Ticker ê°ì²´ ìƒì„± ë° info ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„)
    for attempt in range(3):
        try:
            t = yf.Ticker(tkr)
            info = t.get_info() or {}
            if info:  # infoê°€ ìˆìœ¼ë©´ ì„±ê³µ
                break
        except Exception as e:
            if attempt < 2:
                time.sleep(0.3 + random.uniform(0, 0.3))
            else:
                log_error("GET_INFO", tkr, f"Failed to get info after 3 attempts: {str(e)}")
                # info ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ì¬ë¬´ì œí‘œëŠ” ì‹œë„)
                if t is None:
                    try:
                        t = yf.Ticker(tkr)
                    except:
                        return _create_default_record(tkr, price, avg_vol)

    try:
        mktcap = validate_market_cap(info.get("marketCap"))
        price = validate_price(price)
        avg_vol = validate_volume(avg_vol)
        dollar_vol = (float(price) * float(avg_vol)) if (price is not None and avg_vol is not None) else None

        # ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        q_is = _safe_df(lambda: t.quarterly_income_stmt, max_retries=3)
        if q_is is None:
            q_is = _safe_df(lambda: t.quarterly_financials, max_retries=2)

        a_is = _safe_df(lambda: t.income_stmt, max_retries=3)
        if a_is is None:
            a_is = _safe_df(lambda: t.financials, max_retries=2)

        cf_q = _safe_df(lambda: t.quarterly_cashflow, max_retries=3)
        balance_a = _safe_df(lambda: t.balance_sheet, max_retries=3)

        # ì¬ë¬´ì œí‘œ ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€ ë¡œê¹…
        financial_data_available = sum([
            q_is is not None,
            a_is is not None,
            cf_q is not None,
            balance_a is not None
        ])

        if CONFIG.get("VERBOSE_LOGGING", False) and financial_data_available == 0:
            log_error("FINANCIAL_DATA", tkr, "No financial statements available")

        # ê°•í™”ëœ PER ê³„ì‚° (ì—ëŸ¬ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)
        pe_enhanced = None
        try:
            pe_enhanced = calculate_pe_ratio(tkr, price, info, q_is, a_is)
        except Exception as e:
            log_error("PE_CALC", tkr, f"PE calculation failed: {str(e)}")

        # PEG ê³„ì‚° (ì—ëŸ¬ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰)
        peg_enhanced = None
        try:
            if pe_enhanced and pe_enhanced > 0:
                earnings_growth = info.get("earningsGrowth") or info.get("earningsQuarterlyGrowth")
                if earnings_growth and earnings_growth > 0:
                    peg_enhanced = pe_enhanced / (earnings_growth * 100)
                    peg_enhanced = validate_ratio(peg_enhanced, min_ratio=0, max_ratio=100)
                else:
                    eps_series = get_eps_annual_series(t)
                    eps_cagr = eps_cagr_from_series(eps_series, 3, 5)
                    if eps_cagr and eps_cagr > 0:
                        peg_enhanced = pe_enhanced / (eps_cagr * 100)
                        peg_enhanced = validate_ratio(peg_enhanced, min_ratio=0, max_ratio=100)
        except Exception as e:
            log_error("PEG_CALC", tkr, f"PEG calculation failed: {str(e)}")

        # ê¸°ë³¸ ì¬ë¬´ ë°ì´í„° (ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì—ëŸ¬ ì²˜ë¦¬)
        rev_yoy = op_margin = None
        try:
            rev_yoy, op_margin = _calculate_financial_ratios(q_is, a_is)
        except Exception as e:
            log_error("FINANCIAL_RATIOS", tkr, f"Failed: {str(e)}")

        ev_ebitda = None
        try:
            ev_ebitda = _calculate_ev_ebitda(info, q_is)
        except Exception as e:
            log_error("EV_EBITDA", tkr, f"Failed: {str(e)}")

        fcf_yield = None
        try:
            fcf_yield = _calculate_fcf_yield(info, cf_q)
        except Exception as e:
            log_error("FCF_YIELD", tkr, f"Failed: {str(e)}")

        growth_indicators = {"EPS_Growth_3Y": None, "Revenue_Growth_3Y": None, "EBITDA_Growth_3Y": None}
        try:
            growth_indicators = _calculate_growth_indicators(q_is, a_is, info)
        except Exception as e:
            log_error("GROWTH_INDICATORS", tkr, f"Failed: {str(e)}")

        # ì´ìƒì¹˜ ê²€ì¦
        operating_margins = validate_percentage(info.get("operatingMargins"), min_pct=-1.0, max_pct=1.0)
        roe = validate_percentage(info.get("returnOnEquity"), min_pct=-5.0, max_pct=5.0)
        roa = validate_percentage(info.get("returnOnAssets"), min_pct=-5.0, max_pct=5.0)
        pb = validate_ratio(info.get("priceToBook") or info.get("priceToBookRatio"), min_ratio=0, max_ratio=100)
        ps = validate_ratio(info.get("priceToSalesTrailing12Months"), min_ratio=0, max_ratio=100)
        div_yield = validate_percentage(info.get("dividendYield") or info.get("trailingAnnualDividendYield"), min_pct=0,
                                        max_pct=0.5)
        payout_ratio = validate_percentage(info.get("payoutRatio"), min_pct=0, max_pct=2.0)
        beta = validate_numeric(info.get("beta"), min_val=-5, max_val=5, allow_negative=True)
        short_percent = validate_percentage(info.get("shortPercentOfFloat"), min_pct=0, max_pct=1.0)
        insider_ownership = validate_percentage(info.get("heldPercentInsiders"), min_pct=0, max_pct=1.0)
        institution_ownership = validate_percentage(info.get("heldPercentInstitutions"), min_pct=0, max_pct=1.0)

        # ê¸°ë³¸ ë ˆì½”ë“œ ìƒì„±
        rec = {
            "Ticker": tkr,
            "Name": info.get("longName") or info.get("shortName") or tkr,
            "Sector": info.get("sector"),
            "Industry": info.get("industry"),
            "MktCap($B)": round((mktcap or 0) / 1_000_000_000, 2) if mktcap else None,
            "Price": round(price, 2) if price is not None else None,
            "DollarVol($M)": round((dollar_vol or 0) / 1_000_000, 2) if dollar_vol is not None else None,

            # ì¬ë¬´ ì§€í‘œ
            "RevYoY": rev_yoy,
            "OpMarginTTM": op_margin,
            "OperatingMargins(info)": operating_margins,
            "ROE(info)": roe,
            "ROA(info)": roa,
            "EV_EBITDA": ev_ebitda,
            "PE": pe_enhanced,
            "PEG": peg_enhanced,
            "FCF_Yield": fcf_yield,
            "PB": pb,
            "PS": ps,
            "DivYield": div_yield,
            "PayoutRatio": payout_ratio,

            # ì‹ ê·œ ì„±ì¥ì„± ì§€í‘œ
            **growth_indicators,

            # ê¸°íƒ€
            "Beta": beta,
            "ShortPercent": short_percent,
            "InsiderOwnership": insider_ownership,
            "InstitutionOwnership": institution_ownership,
        }

        # ëˆ„ë½ëœ ë°ì´í„° ê³„ì‚°ìœ¼ë¡œ ë³´ì™„
        try:
            calculated = calculate_missing_financials(tkr, info, q_is, a_is, cf_q, balance_a, price)
            for key, value in calculated.items():
                if rec.get(key) is None and value is not None:
                    rec[key] = value
        except Exception as e:
            log_error("MISSING_FINANCIALS", tkr, f"Failed: {str(e)}")

        return rec

    except Exception as e:
        log_error("FETCH_DETAILS", tkr, f"Unexpected error: {str(e)}")
        return _create_default_record(tkr, price, avg_vol, info)


def _create_default_record(tkr, price, avg_vol, info=None):
    """ê¸°ë³¸ ë ˆì½”ë“œ ìƒì„±"""
    if info is None:
        info = {}

    mktcap = validate_market_cap(info.get("marketCap"))
    price = validate_price(price)
    avg_vol = validate_volume(avg_vol)
    dollar_vol = (float(price) * float(avg_vol)) if (price is not None and avg_vol is not None) else None

    return {
        "Ticker": tkr,
        "Name": info.get("longName") or info.get("shortName") or tkr,
        "Sector": info.get("sector"),
        "Industry": info.get("industry"),
        "MktCap($B)": round((mktcap or 0) / 1_000_000_000, 2) if mktcap else None,
        "Price": round(price, 2) if price is not None else None,
        "DollarVol($M)": round((dollar_vol or 0) / 1_000_000, 2) if dollar_vol is not None else None,
        "RevYoY": None, "OpMarginTTM": None, "OperatingMargins(info)": None,
        "ROE(info)": None, "ROA(info)": None, "EV_EBITDA": None, "PE": None,
        "PEG": None, "FCF_Yield": None, "PB": None, "PS": None, "DivYield": None,
        "PayoutRatio": None,
        "EPS_Growth_3Y": None, "Revenue_Growth_3Y": None, "EBITDA_Growth_3Y": None,
        "Beta": None, "ShortPercent": None, "InsiderOwnership": None, "InstitutionOwnership": None,
    }


def build_enhanced_details_cache():
    """â­ ì™„ì „ ë³‘ë ¬í™”ëœ ìºì‹œ ë¹Œë“œ í•¨ìˆ˜"""
    source = CONFIG["UNIVERSE_SOURCE"]
    tickers = load_universe()

    # OHLCV ë¼ì´íŠ¸ ì§€í‘œ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
    print("\n" + "=" * 60)
    print("ğŸ“Š 1ë‹¨ê³„: OHLCV ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)")
    print("=" * 60)

    TA, PX, VOL, ok = preload_ohlcv_light(
        tickers,
        period=CONFIG["PRELOAD_PERIOD"],
        chunk=CONFIG["PRELOAD_CHUNK"]
    )

    if not ok:
        raise RuntimeError("OHLCV ë¼ì´íŠ¸ í”„ë¦¬ë¡œë“œ ì‹¤íŒ¨")

    # ë¼ì´íŠ¸ í‘œ ìƒì„±
    lite_rows = []
    for t in tickers:
        tta = TA.get(t, {})
        price = PX.get(t)
        avg20 = VOL.get(t)
        if price is None or avg20 is None:
            continue

        dollar_vol = price * avg20
        row = {
            "Ticker": t,
            "Price": round(price, 2),
            "DollarVol($M)": round(dollar_vol / 1_000_000, 2),
            "SMA20": tta.get("sma20"),
            "SMA50": tta.get("sma50"),
            "SMA200": tta.get("sma200"),
            "ATR_PCT": tta.get("atr_pct"),
            "RVOL": tta.get("rvol"),
            "RET5": tta.get("ret5"),
            "RET20": tta.get("ret20"),
            "RET63": tta.get("ret63"),
            "RSI_14": tta.get("rsi_14"),
            "MACD": tta.get("macd"),
            "MACD_Signal": tta.get("macd_signal"),
            "MACD_Histogram": tta.get("macd_histogram"),
            "BB_Position": tta.get("bb_position"),
            "High_52W_Ratio": tta.get("high_52w_ratio"),
            "Low_52W_Ratio": tta.get("low_52w_ratio"),
            "Momentum_12M": tta.get("momentum_12m"),
            "Volatility_21D": tta.get("volatility_21d"),
        }
        lite_rows.append(row)

    lite_df = pd.DataFrame(lite_rows)
    if lite_df.empty:
        raise RuntimeError("ë¼ì´íŠ¸ ì§€í‘œ í‘œê°€ ë¹„ì–´ ìˆìŒ")

    # ìƒì„¸ í˜¸ì¶œ ëŒ€ìƒ ì„ ì •
    lite_df["_pass_light_generic"] = lite_df.apply(
        lambda r: pass_light_generic(r["Price"], r["DollarVol($M)"] * 1_000_000), axis=1
    )

    passed_tickers = lite_df[lite_df["_pass_light_generic"]]
    print(f"\në¼ì´íŠ¸ í•„í„° í†µê³¼: {len(passed_tickers)}ê°œ")

    cand = passed_tickers.sort_values("DollarVol($M)", ascending=False).head(CONFIG["DETAILED_TOP_K"])
    print(f"ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ëŒ€ìƒ: {len(cand)}ê°œ")

    # â­ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìƒì„¸ ì¬ë¬´ ìˆ˜ì§‘
    print("\n" + "=" * 60)
    print("ğŸ’¼ 2ë‹¨ê³„: ìƒì„¸ ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)")
    print("=" * 60)

    detail_rows = []
    success_count = 0
    error_count = 0

    print(f"[ìƒì„¸ë°ì´í„°] {CONFIG['DETAIL_FETCH_WORKERS']}ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

    # ì‘ì—… ì¤€ë¹„
    tasks = [(t, row) for t, row in cand.set_index("Ticker").iterrows()]

    # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=CONFIG["DETAIL_FETCH_WORKERS"]) as executor:
        futures = {executor.submit(fetch_single_ticker_wrapper, task): task[0] for task in tasks}

        for i, future in enumerate(as_completed(futures), start=1):
            ticker = futures[future]
            try:
                rec, error = future.result()

                if rec is not None:
                    detail_rows.append(rec)
                    success_count += 1
                else:
                    error_count += 1

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if (i % 100) == 0:
                    print(f"  - {i}/{len(tasks)} ì™„ë£Œ (ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count})")

            except Exception:
                error_count += 1

    print(f"[ìƒì„¸ë°ì´í„°] ìµœì¢… ìˆ˜ì§‘: {success_count}/{len(cand)} ì¢…ëª© (ì‹¤íŒ¨: {error_count})")

    # ë°ì´í„° ë³‘í•©
    details_df = pd.DataFrame(detail_rows)
    details_dict = details_df.set_index('Ticker').to_dict('index')

    base_df = passed_tickers.drop(columns=["_pass_light_generic"]).copy()
    detail_columns = [col for col in details_df.columns if col not in ['Ticker']]

    for col in detail_columns:
        base_df[col] = base_df['Ticker'].map(
            {ticker: data.get(col) for ticker, data in details_dict.items()}
        )

    out = base_df
    print(f"\nìµœì¢… CSV í–‰ ìˆ˜: {len(out)}")

    # ë°ì´í„° íƒ€ì… ì •ë¦¬
    numeric_columns = ["RevYoY", "OpMarginTTM", "OperatingMargins(info)", "ROE(info)",
                       "FCF_Yield", "DivYield", "EPS_Growth_3Y", "Revenue_Growth_3Y",
                       "RSI_14", "MACD", "MACD_Signal", "MACD_Histogram", "BB_Position",
                       "High_52W_Ratio", "Low_52W_Ratio", "Momentum_12M", "Volatility_21D"]

    for col in numeric_columns:
        if col in out.columns:
            out[col] = pd.to_numeric(out[col], errors='coerce')

    out["CreatedAtUTC"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    out["Source"] = source

    # ì €ì¥
    base = CONFIG["OUT_BASENAME"].strip() or f"details_cache_{source}"
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = f"{base}_{ts}.csv"
    out.to_csv(csv_path, index=False)
    print(f"\n[ìºì‹œ] ì €ì¥ ì™„ë£Œ: {csv_path} (í–‰: {len(out)})")

    if CONFIG["INCLUDE_EXCEL"]:
        try:
            xlsx_path = f"{base}_{ts}.xlsx"
            out.to_excel(xlsx_path, index=False)
            print(f"[ìºì‹œ] ì—‘ì…€ ì €ì¥: {xlsx_path}")
        except Exception as e:
            print(f"[ìºì‹œ] ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
    if ERROR_LOG:
        error_log_path = f"{base}_{ts}_errors.log"
        try:
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"Total errors: {len(ERROR_LOG)}\n")
                f.write("=" * 80 + "\n")
                for error_msg in ERROR_LOG:
                    f.write(error_msg + "\n")
            print(f"[ë¡œê·¸] ì—ëŸ¬ ë¡œê·¸ ì €ì¥: {error_log_path} ({len(ERROR_LOG)}ê°œ ì—ëŸ¬)")
        except Exception as e:
            print(f"[ë¡œê·¸] ì—ëŸ¬ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ë°ì´í„° í’ˆì§ˆ í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ í†µê³„")
    print("=" * 60)

    quality_stats = {
        "PE ìˆìŒ": out["PE"].notna().sum(),
        "PEG ìˆìŒ": out["PEG"].notna().sum(),
        "RevYoY ìˆìŒ": out["RevYoY"].notna().sum(),
        "OpMarginTTM ìˆìŒ": out["OpMarginTTM"].notna().sum(),
        "FCF_Yield ìˆìŒ": out["FCF_Yield"].notna().sum(),
        "ROE ìˆìŒ": out["ROE(info)"].notna().sum(),
        "EV_EBITDA ìˆìŒ": out["EV_EBITDA"].notna().sum(),
    }

    for metric, count in quality_stats.items():
        percentage = (count / len(out) * 100) if len(out) > 0 else 0
        print(f"  {metric}: {count}/{len(out)} ({percentage:.1f}%)")

    print("=" * 60)

    return out


# ============== ë¼ì´íŠ¸ ì»· í•¨ìˆ˜ ==============
def pass_light_generic(price, dollar_vol):
    """1ì°¨ í•„í„°: ë„ˆë¬´ ì•ˆì¢‹ì€ í‹°ì»¤ë§Œ ê±¸ëŸ¬ëƒ„"""
    if price is None or dollar_vol is None:
        return False

    price = validate_price(price)
    dollar_vol = validate_numeric(dollar_vol, min_val=0)

    if price is None or dollar_vol is None:
        return False

    return (price >= CONFIG["MIN_PRICE"]) and (dollar_vol >= CONFIG["MIN_DOLLAR_VOLUME"])


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸš€ ì™„ì „ ìµœì í™”ëœ í‹°ì»¤ ìºì‹œ ë¹Œë” ì‹œì‘")
    print("=" * 60)
    print(f"  âœ… OHLCV ë³‘ë ¬ ìŠ¤ë ˆë“œ: {CONFIG['OHLCV_WORKERS']}ê°œ")
    print(f"  âœ… ìƒì„¸ ë°ì´í„° ë³‘ë ¬ ìŠ¤ë ˆë“œ: {CONFIG['DETAIL_FETCH_WORKERS']}ê°œ")
    print(f"  âœ… ì´ìƒì¹˜ ê²€ì¦: ê°•í™”ë¨")
    print("=" * 60 + "\n")

    start_time = time.time()
    build_enhanced_details_cache()
    elapsed = time.time() - start_time

    print("\n" + "=" * 60)
    print(f"âœ… ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ ({elapsed / 60:.1f}ë¶„)")
    print("=" * 60)