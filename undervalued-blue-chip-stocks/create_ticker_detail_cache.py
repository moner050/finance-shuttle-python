# -*- coding: utf-8 -*-
"""
build_details_cache.py

ìœ ë‹ˆë²„ìŠ¤(US_ALL/SP500/ì»¤ìŠ¤í…€)ë¥¼ ë¶ˆëŸ¬ì™€ OHLCV(ê¸°ë³¸ 120d)ì—ì„œ ë¼ì´íŠ¸ ì§€í‘œ(Price, DollarVol, RVOL, ATR_PCT ë“±)ë¥¼ ì „ì¢…ëª© ì‚°ì¶œ
â†’ ë¼ì´íŠ¸ ì»· í†µê³¼ ì¢…ëª©(ë° ìƒìœ„ DETAILED_TOP_K)ì— í•œí•´ ì¬ë¬´ ì§€í‘œ(RevYoY, OpMarginTTM, EV/EBITDA, FCFY ë“±)ê¹Œì§€ ìˆ˜ì§‘
â†’ ë‹¨ì¼ ìºì‹œ íŒŒì¼(details_cache_{source}.csv / .xlsx)ì— ì €ì¥

ê¶Œì¥ ì„¤ì¹˜:
  pip install -U yfinance==0.2.43 pandas numpy XlsxWriter openpyxl requests matplotlib

ê°œì„ ì‚¬í•­:
1. EV/EBITDA ê³„ì‚° ë¡œì§ ê°•í™”
2. FCF Yield ê³„ì‚° ë°©ì‹ ê°œì„ 
3. ì¬ë¬´ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ
4. ì—ëŸ¬ ì²˜ë¦¬ ë³´ì™„
"""

import os, io, time, math, random, warnings, logging, requests
import pandas as pd, numpy as np, yfinance as yf
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
logging.getLogger("yfinance").setLevel(logging.CRITICAL)

# ===================== CONFIG =====================
CONFIG = {
    "UNIVERSE_SOURCE": "us_all",  # "us_all" | "sp500" | "custom"
    "CUSTOM_TICKERS": [],  # UNIVERSE_SOURCE="custom"ì¼ ë•Œ ì‚¬ìš©

    # ìºì‹œ ì¶œë ¥
    "OUT_BASENAME": "",  # ë¹„ìš°ë©´ ìë™: details_cache_{source}.csv
    "INCLUDE_EXCEL": True,

    # OHLCV í”„ë¦¬ë¡œë“œ(ë¼ì´íŠ¸ ì§€í‘œ)
    "PRELOAD_PERIOD": "120d",
    "PRELOAD_CHUNK": 50,  # ì²­í¬ í¬ê¸° ì¤„ì„
    "BATCH_RETRIES": 5,  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
    "SINGLE_RETRIES": 3,
    "FALLBACK_MAX_WORKERS": 8,  # ì›Œì»¤ ìˆ˜ ì¤„ì„
    "YF_THREADS": False,  # ìŠ¤ë ˆë“œ ë¹„í™œì„±í™” (ì•ˆì •ì„±)
    "SLEEP_SEC": 0.25,  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€

    # ë„¤íŠ¸ì›Œí¬ ì„¤ì •
    "REQUEST_TIMEOUT": 60,
    "PROXY_SETTINGS": {},  # {'http': 'http://proxy:port', 'https': 'https://proxy:port'}

    # ë¼ì´íŠ¸ ì»·(ë¼ì´íŠ¸ í†µê³¼ ì¢…ëª©ë§Œ ìƒì„¸ ì¬ë¬´ í˜¸ì¶œ)
    "MIN_PRICE": 1.0,
    "MIN_DOLLAR_VOLUME": 900_000,

    # ìƒì„¸ ì¬ë¬´ í˜¸ì¶œ ëŒ€ìƒ ë²”ìœ„
    "DETAILED_TOP_K": 12000,  # ìƒìœ„ K ì¤„ì„
    "MAX_TICKERS": 12000,  # ìµœëŒ€ í‹°ì»¤ ìˆ˜ ì¤„ì„ (ì²˜ë¦¬ ì†ë„ í–¥ìƒ)
    "UNIVERSE_OFFSET": 0,
    "SHUFFLE_UNIVERSE": True,

    # ë²„í•í˜• í•˜ë“œì»· ê¸°ë³¸ì„ 
    "MIN_MKTCAP": 800_000_000,

    # ìš”ì²­ í—¤ë”
    "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
}
# ==================================================

HEADERS = {"User-Agent": CONFIG["USER_AGENT"]}
HTTP_SESSION = requests.Session()
HTTP_SESSION.headers.update(HEADERS)

# ì„¸ì…˜ ì„¤ì •
session = requests.Session()
session.headers.update({"User-Agent": CONFIG["USER_AGENT"]})
if CONFIG["PROXY_SETTINGS"]:
    session.proxies.update(CONFIG["PROXY_SETTINGS"])

def _normalize_ticker(t):
    return str(t).strip().upper().replace(".", "-")

def _read_html(url: str):
    try:
        r = session.get(url, timeout=CONFIG["REQUEST_TIMEOUT"])
        r.raise_for_status()
        return pd.read_html(io.StringIO(r.text))
    except Exception as e:
        print(f"HTML ì½ê¸° ì‹¤íŒ¨ {url}: {e}")
        return []


def get_sp500_symbols():
    """S&P 500 ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    urls = [
        "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies",
        "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/main/data/constituents.csv"
    ]

    for url in urls:
        try:
            if "wikipedia" in url:
                tables = _read_html(url)
                if tables:
                    df = tables[0]
                    col = next((c for c in df.columns if str(c).lower().startswith("symbol")), "Symbol")
                    syms = df[col].dropna().astype(str).tolist()
                    print(f"[S&P500] Wikipediaì—ì„œ {len(syms)}ê°œ ì¢…ëª© ë¡œë“œ")
                    return [_normalize_ticker(s) for s in syms]
            else:
                r = session.get(url, timeout=CONFIG["REQUEST_TIMEOUT"])
                r.raise_for_status()
                df = pd.read_csv(io.StringIO(r.text))
                if 'Symbol' in df.columns:
                    syms = df['Symbol'].dropna().astype(str).tolist()
                    print(f"[S&P500] GitHubì—ì„œ {len(syms)}ê°œ ì¢…ëª© ë¡œë“œ")
                    return [_normalize_ticker(s) for s in syms]
        except Exception as e:
            print(f"[S&P500] {url} ì‹¤íŒ¨: {e}")
            continue

    # í´ë°±: í•˜ë“œì½”ë”©ëœ ì£¼ìš” S&P 500 ì¢…ëª©
    fallback_sp500 = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'GOOG', 'TSLA', 'BRK-B', 'UNH', 'JNJ', 'XOM',
                      'JPM', 'V', 'NVDA', 'PG', 'MA', 'HD', 'CVX', 'LLY', 'ABBV', 'PFE']
    print(f"[S&P500] í´ë°±: {len(fallback_sp500)}ê°œ ì£¼ìš” ì¢…ëª© ì‚¬ìš©")
    return fallback_sp500

def _fetch_text(url):
    try:
        r = session.get(url, timeout=CONFIG["REQUEST_TIMEOUT"], allow_redirects=True)
        r.raise_for_status()
        return r.text
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {url}: {e}")
        return ""

def _read_pipe_text_to_df(text: str) -> pd.DataFrame:
    try:
        return pd.read_csv(io.StringIO(text), sep="|")
    except Exception as e:
        print(f"íŒŒì´í”„ í…ìŠ¤íŠ¸ ì½ê¸° ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def _normalize_symbol_df(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    cols = {c.lower(): c for c in df.columns}
    sym = cols.get("symbol") or cols.get("act symbol") or cols.get("nasdaq symbol") or list(df.columns)[0]

    out = df.copy()
    out.rename(columns={sym: "Symbol"}, inplace=True)
    out["Symbol"] = out["Symbol"].astype(str).str.upper().str.replace(".", "-", regex=False)

    # ê¸°ë³¸ í•„í„°ë§
    if "TestIssue" in out.columns:
        mask_test = out["TestIssue"].astype(str).str.upper().ne("Y")
        out = out[mask_test]

    if "FinancialStatus" in out.columns:
        fin_s = out["FinancialStatus"].astype(str).str.upper()
        mask_fin = (~fin_s.isin(["D", "E", "H", "S", "C", "T"]))
        out = out[mask_fin]

    return out


def _filter_common_stock(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    name_str = df.get("SecurityName", pd.Series([""] * len(df))).astype(str).str.lower()

    is_common_kw = name_str.str.contains(
        r"common stock|ordinary shares|class [ab]\s+common|shs",
        regex=True, na=False
    )
    is_deriv_kw = name_str.str.contains(
        r"warrant|right|unit|preferred|preference|pref|etf|fund|trust|note|debenture|bond|adr|adr\.",
        regex=True, na=False
    )

    base = df[is_common_kw & ~is_deriv_kw]
    return base if not base.empty else df[~is_deriv_kw]


def get_all_us_listed_common():
    """ëª¨ë“  ë¯¸êµ­ ìƒì¥ ì£¼ì‹ ì¢…ëª© ê°€ì ¸ì˜¤ê¸° - ê°œì„ ëœ ë²„ì „"""
    urls = [
        "https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt",
        "https://www.nasdaqtrader.com/dynamic/SymDir/otherlisted.txt",
        "https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download",
        "https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download",
    ]

    dfs = []
    fetched_count = 0

    for u in urls:
        try:
            txt = _fetch_text(u)
            if not txt:
                continue

            df = _normalize_symbol_df(_read_pipe_text_to_df(txt))
            if not df.empty:
                dfs.append(df)
                fetched_count += len(df)
                print(f"[US_ALL] {u}ì—ì„œ {len(df)}ê°œ ì¢…ëª© ë¡œë“œ")

        except Exception as e:
            print(f"[US_ALL] {u} ê±´ë„ˆëœ€: {e}")
            continue

    if not dfs:
        print("[US_ALL] ëª¨ë“  ì†ŒìŠ¤ ì‹¤íŒ¨, í´ë°± ì¢…ëª© ì‚¬ìš©")
        # ê¸°ë³¸ í´ë°± ì¢…ëª©
        fallback_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'BRK-B', 'V', 'JNJ', 'WMT',
                            'PG', 'JPM', 'UNH', 'HD', 'DIS', 'PYPL', 'NFLX', 'ADBE', 'CRM', 'INTC']
        return fallback_tickers

    df_combined = pd.concat(dfs, ignore_index=True)
    df_combined = _filter_common_stock(df_combined)

    syms = df_combined["Symbol"].dropna().unique().tolist()
    print(f"[US_ALL] í•„í„° í›„ ì´ {len(syms)}ê°œ ì¢…ëª©")
    return sorted(syms)


def load_universe():
    """ìœ ë‹ˆë²„ìŠ¤ ë¡œë“œ - ì•ˆì •ì„± ê°œì„ """
    src = CONFIG["UNIVERSE_SOURCE"]

    try:
        if src == "sp500":
            u = get_sp500_symbols()
        elif src == "us_all":
            u = get_all_us_listed_common()
        elif src == "custom":
            u = [_normalize_ticker(x) for x in CONFIG["CUSTOM_TICKERS"]]
        else:
            raise ValueError("UNIVERSE_SOURCEëŠ” us_all, sp500, custom ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")

        if CONFIG["SHUFFLE_UNIVERSE"]:
            random.shuffle(u)

        if CONFIG["MAX_TICKERS"]:
            u = u[CONFIG["UNIVERSE_OFFSET"]:CONFIG["UNIVERSE_OFFSET"] + CONFIG["MAX_TICKERS"]]
        elif CONFIG["UNIVERSE_OFFSET"]:
            u = u[CONFIG["UNIVERSE_OFFSET"]:]

        print(f"[ìœ ë‹ˆë²„ìŠ¤] {src} ì´={len(u)}ê°œ ìƒ˜í”Œ={u[:8]}")
        return u

    except Exception as e:
        print(f"[ìœ ë‹ˆë²„ìŠ¤] ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì¢…ëª©ìœ¼ë¡œ í´ë°±
        return ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'BRK-B', 'JNJ', 'JPM', 'V']

# ============== OHLCV â†’ ë¼ì´íŠ¸ ì§€í‘œ ==============

def _compute_ta_single(c, h, l, v):
    """ë‹¨ì¼ ì¢…ëª© ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    try:
        if c is None or len(c.dropna()) < 5:
            return None

        c = c.dropna()
        if len(c) == 0:
            return None

        # ê¸°ë³¸ ì§€í‘œ ê³„ì‚°
        last_close = float(c.iloc[-1])

        # ì´ë™í‰ê· 
        s20 = c.rolling(20).mean().iloc[-1] if len(c) >= 20 else None
        s50 = c.rolling(50).mean().iloc[-1] if len(c) >= 50 else None

        # ìˆ˜ìµë¥ 
        ret5 = c.pct_change(5).iloc[-1] if len(c) >= 6 else None
        ret20 = c.pct_change(20).iloc[-1] if len(c) >= 21 else None

        # ê±°ë˜ëŸ‰ ì§€í‘œ
        avg20_vol = today_vol = rvol = None
        if v is not None and len(v.dropna()) > 0:
            v_clean = v.dropna()
            avg20_vol = float(v_clean.rolling(20).mean().iloc[-1]) if len(v_clean) >= 20 else float(v_clean.mean())
            today_vol = float(v_clean.iloc[-1]) if len(v_clean) > 0 else None
            rvol = today_vol / avg20_vol if avg20_vol and avg20_vol > 0 else 1.0

        # ATR
        atr = atr_pct = None
        if h is not None and l is not None and len(h.dropna()) > 0 and len(l.dropna()) > 0:
            h_clean, l_clean = h.dropna(), l.dropna()
            if len(h_clean) >= 2 and len(l_clean) >= 2:
                prev_close = c.shift(1)
                tr = pd.concat([
                    h_clean - l_clean,
                    (h_clean - prev_close).abs(),
                    (l_clean - prev_close).abs()
                ], axis=1).max(axis=1)
                atr = float(tr.rolling(14).mean().iloc[-1]) if len(tr) >= 14 else None
                atr_pct = (atr / last_close) if atr and last_close > 0 else None

        return {
            "last_price": last_close,
            "sma20": float(s20) if s20 else None,
            "sma50": float(s50) if s50 else None,
            "ret5": float(ret5) if ret5 else None,
            "ret20": float(ret20) if ret20 else None,
            "avg20_vol": avg20_vol,
            "today_vol": today_vol,
            "rvol": rvol,
            "atr": atr,
            "atr_pct": atr_pct
        }
    except Exception as e:
        print(f"TA ê³„ì‚° ì‹¤íŒ¨: {e}")
        return None

def _compute_ta_metrics(df):
    """DataFrameì—ì„œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    out = {}

    try:
        if isinstance(df.columns, pd.MultiIndex):
            # ë©€í‹°ì¸ë±ìŠ¤ (ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ)
            fields = set(df.columns.get_level_values(0))
            tickers = sorted(set(df.columns.get_level_values(1)))

            close_col = "Adj Close" if "Adj Close" in fields else "Close"

            for t in tickers:
                try:
                    if (close_col, t) not in df.columns:
                        continue

                    c = df[(close_col, t)].dropna()
                    h = df[("High", t)].dropna() if ("High", t) in df.columns else None
                    l = df[("Low", t)].dropna() if ("Low", t) in df.columns else None
                    v = df[("Volume", t)].dropna() if ("Volume", t) in df.columns else None

                    metrics = _compute_ta_single(c, h, l, v)
                    if metrics:
                        out[t] = metrics
                except Exception:
                    continue
        else:
            # ë‹¨ì¼ ì¸ë±ìŠ¤
            close_col = "Adj Close" if "Adj Close" in df.columns else "Close"
            c = df[close_col] if close_col in df.columns else None
            h = df["High"] if "High" in df.columns else None
            l = df["Low"] if "Low" in df.columns else None
            v = df["Volume"] if "Volume" in df.columns else None

            metrics = _compute_ta_single(c, h, l, v)
            if metrics:
                out["__SINGLE__"] = metrics

    except Exception as e:
        print(f"TA ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")

    return out

def safe_yf_download(tickers, **kwargs):
    """ì•ˆì „í•œ yfinance ë‹¤ìš´ë¡œë“œ"""
    max_retries = kwargs.pop('max_retries', 3)
    timeout = kwargs.pop('timeout', 30)

    for attempt in range(max_retries):
        try:
            data = yf.download(tickers, **kwargs)
            if data is not None and not data.empty:
                return data
        except Exception as e:
            print(f"yfinance ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨: {e}")
            if attempt < max_retries - 1:
                sleep_time = (2 ** attempt) + random.random()
                print(f"{sleep_time:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                time.sleep(sleep_time)

    return None


def preload_ohlcv_light(tickers, period="120d", chunk=50, **kwargs):
    """OHLCV ë°ì´í„° í”„ë¦¬ë¡œë“œ - ê°œì„ ëœ ë²„ì „"""
    TA, PX, VOL = {}, {}, {}
    ok_tickers = set()

    print(f"[OHLCV] {len(tickers)}ê°œ ì¢…ëª© ë¡œë“œ ì‹œì‘...")

    for i in range(0, len(tickers), chunk):
        batch = tickers[i:i + chunk]
        batch_name = f"{i + 1}-{min(i + chunk, len(tickers))}"

        print(f"[OHLCV] ë°°ì¹˜ {batch_name} ì²˜ë¦¬ ì¤‘...")

        # ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹œë„
        batch_data = None
        for attempt in range(CONFIG["BATCH_RETRIES"]):
            try:
                batch_data = safe_yf_download(
                    batch,
                    period=period,
                    interval="1d",
                    auto_adjust=False,
                    progress=False,
                    threads=CONFIG["YF_THREADS"],
                    timeout=30
                )
                if batch_data is not None and not batch_data.empty:
                    break
            except Exception as e:
                print(f"ë°°ì¹˜ {batch_name} ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                time.sleep((1.5 ** attempt) + random.random())

        processed_in_batch = 0

        if batch_data is not None and not batch_data.empty:
            # ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
            metrics = _compute_ta_metrics(batch_data)

            if isinstance(batch_data.columns, pd.MultiIndex):
                # ë©€í‹°ì¸ë±ìŠ¤ ì²˜ë¦¬
                close_col = "Adj Close" if "Adj Close" in set(batch_data.columns.get_level_values(0)) else "Close"

                for t in batch:
                    try:
                        if (close_col, t) not in batch_data.columns:
                            continue

                        prices = batch_data[(close_col, t)].dropna()
                        if len(prices) < 5:
                            continue

                        last_price = float(prices.iloc[-1])

                        # ê±°ë˜ëŸ‰ ê³„ì‚°
                        avg_vol = 0
                        if ("Volume", t) in batch_data.columns:
                            vols = batch_data[("Volume", t)].dropna()
                            avg_vol = float(vols.rolling(20).mean().iloc[-1]) if len(vols) >= 20 else float(vols.mean())

                        ok_tickers.add(t)
                        PX[t] = last_price
                        VOL[t] = avg_vol

                        if t in metrics:
                            TA[t] = metrics[t]
                        else:
                            # ë©”íŠ¸ë¦­ìŠ¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
                            TA[t] = {
                                "last_price": last_price,
                                "sma20": last_price,
                                "sma50": last_price,
                                "ret5": 0.0,
                                "ret20": 0.0,
                                "avg20_vol": avg_vol,
                                "rvol": 1.0,
                                "atr_pct": 0.02
                            }

                        processed_in_batch += 1

                    except Exception as e:
                        print(f"ì¢…ëª© {t} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
            else:
                # ë‹¨ì¼ ì¢…ëª© ì²˜ë¦¬
                if batch and len(batch) == 1:
                    t = batch[0]
                    try:
                        prices = batch_data[close_col].dropna() if close_col in batch_data.columns else None
                        if prices is None or len(prices) < 5:
                            continue

                        last_price = float(prices.iloc[-1])

                        # ê±°ë˜ëŸ‰
                        avg_vol = 0
                        if "Volume" in batch_data.columns:
                            vols = batch_data["Volume"].dropna()
                            avg_vol = float(vols.rolling(20).mean().iloc[-1]) if len(vols) >= 20 else float(vols.mean())

                        ok_tickers.add(t)
                        PX[t] = last_price
                        VOL[t] = avg_vol

                        if "__SINGLE__" in metrics:
                            TA[t] = metrics["__SINGLE__"]
                        else:
                            TA[t] = {
                                "last_price": last_price,
                                "sma20": last_price,
                                "sma50": last_price,
                                "ret5": 0.0,
                                "ret20": 0.0,
                                "avg20_vol": avg_vol,
                                "rvol": 1.0,
                                "atr_pct": 0.02
                            }

                        processed_in_batch += 1

                    except Exception as e:
                        print(f"ë‹¨ì¼ ì¢…ëª© {t} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ê°œë³„ ë‹¤ìš´ë¡œë“œ
        if processed_in_batch == 0:
            print(f"ë°°ì¹˜ {batch_name} ì‹¤íŒ¨, ê°œë³„ ë‹¤ìš´ë¡œë“œ ì‹œë„...")

            def download_single(t):
                for attempt in range(CONFIG["SINGLE_RETRIES"]):
                    try:
                        data = safe_yf_download(
                            t,
                            period=period,
                            interval="1d",
                            auto_adjust=False,
                            progress=False,
                            threads=False,
                            timeout=30
                        )
                        if data is not None and not data.empty:
                            return t, data
                    except Exception:
                        time.sleep((1.5 ** attempt) + random.random() * 0.3)
                return t, None

            # ìŠ¤ë ˆë“œ í’€ ì‚¬ìš©
            with ThreadPoolExecutor(max_workers=CONFIG["FALLBACK_MAX_WORKERS"]) as executor:
                futures = [executor.submit(download_single, t) for t in batch]

                for future in as_completed(futures):
                    t, data = future.result()
                    if data is not None:
                        try:
                            metrics = _compute_ta_metrics(data)
                            if "__SINGLE__" in metrics:
                                close_col = "Adj Close" if "Adj Close" in data.columns else "Close"
                                prices = data[close_col].dropna()

                                if len(prices) >= 5:
                                    last_price = float(prices.iloc[-1])

                                    # ê±°ë˜ëŸ‰
                                    avg_vol = 0
                                    if "Volume" in data.columns:
                                        vols = data["Volume"].dropna()
                                        avg_vol = float(vols.rolling(20).mean().iloc[-1]) if len(vols) >= 20 else float(
                                            vols.mean())

                                    ok_tickers.add(t)
                                    PX[t] = last_price
                                    VOL[t] = avg_vol
                                    TA[t] = metrics["__SINGLE__"]
                                    processed_in_batch += 1
                        except Exception as e:
                            print(f"ê°œë³„ ì¢…ëª© {t} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        print(f"[OHLCV] ë°°ì¹˜ {batch_name} ì™„ë£Œ: {processed_in_batch}/{len(batch)}ê°œ ì„±ê³µ")
        time.sleep(CONFIG["SLEEP_SEC"])

    print(f"[OHLCV] ì „ì²´ ì™„ë£Œ: {len(ok_tickers)}/{len(tickers)}ê°œ ì¢…ëª© ì„±ê³µ")
    return TA, PX, VOL, ok_tickers

# ============== ìƒì„¸ ì¬ë¬´ ìœ í‹¸ ==============
REV_ALIASES = ["total revenue","revenues","revenue","net sales","sales","total net sales"]
OP_ALIASES  = ["operating income","operating income (loss)","income from operations","operating profit","operating profit (loss)","ebit"]
FCF_ALIASES = ["free cash flow","free cashflow","freecashflow"]
DA_ALIASES  = ["depreciation","depreciation & amortization","depreciation and amortization"]
EPS_ALIASES = ["diluted eps","basic eps","eps (diluted)","eps (basic)","earnings per share","eps"]
NET_INCOME_ALIASES = ["net income","net income common stockholders","net income applicable to common shares"]
DIL_SHARES_ALIASES = ["diluted average shares","weighted average shares diluted","weighted average diluted shares outstanding","weighted average diluted shares","weighted average shares - diluted","weighted average number of shares diluted"]
FIN_SECTORS  = {"banks","financial","insurance","capital markets"}
REIT_SECTORS = {"reit","real estate","property"}

def _find_row(index_like, aliases, exclude=None):
    if index_like is None: return None
    exclude=[w.lower() for w in (exclude or [])]
    idx=[str(x).lower() for x in index_like]
    for key in aliases:
        k=key.lower()
        for i,s in enumerate(idx):
            if k in s and not any(x in s for x in exclude):
                return index_like[i]
    return None

def coalesce(*vals):
    for v in vals:
        try:
            if v is None: continue
            if isinstance(v,float) and math.isnan(v): continue
            return v
        except Exception: continue
    return None

def ttm_sum(df: pd.DataFrame, row, n=4, absolute=False):
    if df is None or df.empty or row not in df.index or df.shape[1]<n: return None
    cols=sorted(df.columns, reverse=True)[:n]
    try:
        vals=pd.to_numeric(df.loc[row, cols], errors="coerce").fillna(0)
        return float(vals.abs().sum()) if absolute else float(vals.sum())
    except Exception: return None

def ttm_yoy_growth(df_q: pd.DataFrame, row):
    if df_q is None or df_q.empty or row not in df_q.index or df_q.shape[1]<8: return None
    cols=sorted(df_q.columns, reverse=True)
    try:
        curr=float(pd.to_numeric(df_q.loc[row, cols[:4]], errors="coerce").fillna(0).sum())
        prev=float(pd.to_numeric(df_q.loc[row, cols[4:8]], errors="coerce").fillna(0).sum())
    except Exception: return None
    if prev<=0: return None
    return (curr/prev)-1.0

def annual_yoy_growth(df_a: pd.DataFrame, row):
    if df_a is None or df_a.empty or row not in df_a.index or df_a.shape[1]<2: return None
    cols=sorted(df_a.columns, reverse=True)[:2]
    try:
        curr=float(pd.to_numeric(df_a.loc[row, cols[0]], errors="coerce"))
        prev=float(pd.to_numeric(df_a.loc[row, cols[1]], errors="coerce"))
    except Exception: return None
    if prev<=0: return None
    return (curr/prev)-1.0

def _last4_sum_row(df, aliases):
    if df is None or df.empty: return None
    row=_find_row(df.index, aliases)
    if not row or df.shape[1]<4: return None
    cols=sorted(df.columns, reverse=True)[:4]
    return float(pd.to_numeric(df.loc[row, cols], errors="coerce").fillna(0).sum())

def _last_col(df, aliases):
    if df is None or df.empty: return None
    row=_find_row(df.index, aliases)
    if not row: return None
    col=sorted(df.columns, reverse=True)[0]
    return float(pd.to_numeric(df.loc[row, col], errors="coerce"))

def _eps_ttm_from_statements(df_q, df_a):
    ni=_last4_sum_row(df_q, NET_INCOME_ALIASES)
    sh=_last_col(df_a, DIL_SHARES_ALIASES)
    if ni and sh and sh>0: return ni/sh
    return None

def safe_pe(price, info_dict, df_q, df_a):
    pe=coalesce(info_dict.get("trailingPE"), info_dict.get("forwardPE"))
    if pe is not None and isinstance(pe,(int,float)) and pe>0: return float(pe)
    teps=info_dict.get("trailingEps")
    if teps and isinstance(teps,(int,float)) and teps>0 and price: return float(price)/float(teps)
    eps_ttm=_eps_ttm_from_statements(df_q, df_a)
    if eps_ttm and eps_ttm>0 and price: return float(price)/float(eps_ttm)
    return None

def _parse_growth_to_pct(val):
    if val is None: return None
    try:
        if isinstance(val,str):
            s=val.strip().replace('%','').replace('+','')
            if s.lower() in {'n/a','na','nan','none','-',''}: return None
            return float(s)
        x=float(val); return x*100.0 if abs(x)<=1.0 else x
    except Exception: return None

def estimate_peg_from_earnings_trend(tic: yf.Ticker, pe_value):
    if pe_value is None or pe_value <= 0: return None
    et=None
    for attr in ("earnings_trend","get_earnings_trend"):
        try:
            et=getattr(tic,attr); et=et() if callable(et) else et; break
        except Exception: continue
    growth_pct=None
    if isinstance(et,pd.DataFrame) and ("period" in et.columns) and ("growth" in et.columns):
        for key in ["+5y","5y","next 5y","+1y","1y"]:
            row=et.loc[et["period"].astype(str).str.lower().str.contains(key,na=False)]
            if not row.empty:
                growth_pct=_parse_growth_to_pct(row["growth"].iloc[0]); break
    if growth_pct is None:
        try:
            info=tic.get_info() or {}
            g=info.get("earningsGrowth") or info.get("earningsQuarterlyGrowth")
            growth_pct=_parse_growth_to_pct(g)
        except Exception: pass
    if growth_pct and growth_pct>0:
        return float(pe_value)/float(growth_pct)
    return None

def get_eps_annual_series(tic: yf.Ticker):
    eps_vals=[]; df_a=None
    try:
        df_a=tic.income_stmt
        if df_a is None or df_a.empty: df_a=tic.financials
    except Exception: pass
    if df_a is not None and not df_a.empty:
        row_eps=_find_row(df_a.index, EPS_ALIASES)
        if row_eps:
            try:
                vals=pd.to_numeric(df_a.loc[row_eps], errors="coerce").dropna()
                return list(vals.sort_index().values)
            except Exception: pass
        else:
            ni_row=_find_row(df_a.index, NET_INCOME_ALIASES)
            sh_row=_find_row(df_a.index, DIL_SHARES_ALIASES)
            if ni_row and sh_row:
                try:
                    ni=pd.to_numeric(df_a.loc[ni_row], errors="coerce")
                    sh=pd.to_numeric(df_a.loc[sh_row], errors="coerce").replace(0,np.nan)
                    vals=(ni/sh).dropna()
                    return list(vals.sort_index().values)
                except Exception: pass
    try:
        earn=tic.earnings
        if earn is not None and not earn.empty:
            info={}
            try: info=tic.get_info() or {}
            except Exception: pass
            so=info.get("sharesOutstanding")
            if so and so>0:
                ser=pd.to_numeric(earn["Earnings"], errors="coerce")/float(so)
                return list(ser.sort_index().dropna().values)
    except Exception: pass
    return []

def eps_cagr_from_series(vals, min_years=3, max_years=5):
    v=[float(x) for x in vals if x is not None and not np.isnan(x)]
    if len(v)<min_years: return None
    use=v[-max_years:];
    if len(use)<min_years: return None
    first,last=use[0],use[-1]
    if first<=0 or last<=0: return None
    years=len(use)-1
    if years<=0: return None
    return (last/first)**(1.0/years)-1.0

def estimate_peg_from_eps_cagr(tic: yf.Ticker, pe_value, min_years=3, max_years=5):
    if pe_value is None or pe_value<=0: return None
    cagr=eps_cagr_from_series(get_eps_annual_series(tic), min_years, max_years)
    if cagr is None or cagr<=0: return None
    return float(pe_value)/(float(cagr)*100.0)

def _safe_df(getter):
    """
    ìˆ˜ì •ëœ ë²„ì „: DataFrame boolean í‰ê°€ ë¬¸ì œ í•´ê²°
    """
    try:
        df = getter()
        # ëª…ì‹œì ìœ¼ë¡œ DataFrame ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if df is not None and hasattr(df, 'empty') and not df.empty:
            return df
    except Exception as e:
        # ì—ëŸ¬ ë¡œê¹… (ì„ íƒì‚¬í•­)
        pass
    return None

def safe_pe(price, info_dict, df_q, df_a):
    """
    PER ê³„ì‚° í•¨ìˆ˜ ë³´ì™„
    """
    try:
        pe = coalesce(info_dict.get("trailingPE"), info_dict.get("forwardPE"))
        if pe is not None and isinstance(pe, (int, float)) and pe > 0:
            return float(pe)

        teps = info_dict.get("trailingEps")
        if teps and isinstance(teps, (int, float)) and teps > 0 and price:
            return float(price) / float(teps)

        # ìˆ˜ì •: DataFrame ì•ˆì „ì„± í™•ì¸
        if df_q is not None and df_a is not None:
            eps_ttm = _eps_ttm_from_statements(df_q, df_a)
            if eps_ttm and eps_ttm > 0 and price:
                return float(price) / float(eps_ttm)

        return None
    except Exception:
        return None


def fetch_details_for_ticker(tkr, price, avg_vol):
    """
    ìˆ˜ì •ëœ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
    """
    try:
        t = yf.Ticker(tkr)
        info = t.get_info() or {}
    except Exception as e:
        # ì´ˆê¸° ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
        return {
            "Ticker": tkr,
            "Name": tkr,
            "Sector": None,
            "Industry": None,
            "MktCap($B)": None,
            "Price": round(price, 2) if price is not None else None,
            "DollarVol($M)": None,
            "RevYoY": None,
            "OpMarginTTM": None,
            "OperatingMargins(info)": None,
            "ROE(info)": None,
            "EV_EBITDA": None,
            "PE": None,
            "PEG": None,
            "FCF_Yield": None,
            "PB": None,
            "DivYield": None,
            "PayoutRatio": None,
        }

    try:
        mktcap = info.get("marketCap")
        dollar_vol = (float(price) * float(avg_vol)) if (price is not None and avg_vol is not None) else None

        # ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
        q_is = _safe_df(lambda: t.quarterly_income_stmt)
        if q_is is None:
            q_is = _safe_df(lambda: t.quarterly_financials)

        a_is = _safe_df(lambda: t.income_stmt)
        if a_is is None:
            a_is = _safe_df(lambda: t.financials)

        cf_q = _safe_df(lambda: t.quarterly_cashflow)
        balance_a = _safe_df(lambda: t.balance_sheet)

        # ë§¤ì¶œ/ì˜ì—…ì´ìµ ë°ì´í„° (ì•ˆì „í•œ ì²˜ë¦¬)
        rev_yoy = None
        op_margin = None

        # ìˆ˜ì •: DataFrame ì¡´ì¬ ì—¬ë¶€ ëª…ì‹œì  í™•ì¸
        if q_is is not None and not q_is.empty:
            rev_row = _find_row(q_is.index, REV_ALIASES, exclude=["per share", "operating revenue", "royalty"])
            op_row = _find_row(q_is.index, OP_ALIASES)

            if rev_row:
                rev_ttm = ttm_sum(q_is, rev_row, 4)
                rev_yoy = ttm_yoy_growth(q_is, rev_row)

                # ì—°ê°„ ë°ì´í„°ë¡œ í´ë°±
                if rev_yoy is None and a_is is not None and not a_is.empty and rev_row in a_is.index:
                    rev_yoy = annual_yoy_growth(a_is, rev_row)

            if op_row and rev_ttm and rev_ttm > 0:
                op_ttm = ttm_sum(q_is, op_row, 4)
                if op_ttm:
                    op_margin = op_ttm / rev_ttm

        # EV/EBITDA ê³„ì‚° (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
        ev = info.get("enterpriseValue")
        ebitda = info.get("ebitda")
        ev_ebitda = None

        try:
            if ev and ebitda and float(ebitda) > 0:
                ev_ebitda = float(ev) / float(ebitda)
        except (TypeError, ValueError):
            pass

        # PER/PEG ê³„ì‚°
        pe = safe_pe(price, info, q_is, a_is)
        peg = info.get("pegRatio")

        if (peg is None or math.isnan(peg)) and pe is not None:
            try:
                peg = estimate_peg_from_earnings_trend(t, pe) or estimate_peg_from_eps_cagr(t, pe, 3, 5)
            except Exception:
                peg = None

        # FCF Yield ê³„ì‚°
        fcf_yield = None
        if cf_q is not None and not cf_q.empty:
            fcf_row = _find_row(cf_q.index, FCF_ALIASES)
            if fcf_row:
                fcf_ttm = ttm_sum(cf_q, fcf_row, 4)
                if fcf_ttm and ev and float(ev) > 0:
                    fcf_yield = float(fcf_ttm) / float(ev)

        # ë°°ë‹¹ìˆ˜ìµë¥  (ì•ˆì „í•œ ì²˜ë¦¬)
        div_yield = None
        try:
            div_yield = info.get("dividendYield") or info.get("trailingAnnualDividendYield")
            if div_yield and isinstance(div_yield, str):
                div_yield = float(div_yield.strip('%')) / 100
        except (TypeError, ValueError, AttributeError):
            div_yield = None

        rec = {
            "Ticker": tkr,
            "Name": info.get("longName") or info.get("shortName") or tkr,
            "Sector": info.get("sector"),
            "Industry": info.get("industry"),
            "MktCap($B)": round((mktcap or 0) / 1_000_000_000, 2) if mktcap else None,
            "Price": round(price, 2) if price is not None else None,
            "DollarVol($M)": round((dollar_vol or 0) / 1_000_000, 2) if dollar_vol is not None else None,
            "RevYoY": rev_yoy,
            "OpMarginTTM": op_margin,
            "OperatingMargins(info)": info.get("operatingMargins"),
            "ROE(info)": info.get("returnOnEquity"),
            "EV_EBITDA": ev_ebitda,
            "PE": pe,
            "PEG": peg,
            "FCF_Yield": fcf_yield,
            "PB": info.get("priceToBook") or info.get("priceToBookRatio"),
            "DivYield": div_yield,
            "PayoutRatio": info.get("payoutRatio"),
        }
        return rec

    except Exception as e:
        # ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
        print(f"ì¢…ëª© {tkr} ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {str(e)}")
        return {
            "Ticker": tkr,
            "Name": info.get("longName") or info.get("shortName") or tkr,
            "Sector": info.get("sector"),
            "Industry": info.get("industry"),
            "MktCap($B)": round((mktcap or 0) / 1_000_000_000, 2) if mktcap else None,
            "Price": round(price, 2) if price is not None else None,
            "DollarVol($M)": round((dollar_vol or 0) / 1_000_000, 2) if dollar_vol is not None else None,
            "RevYoY": None,
            "OpMarginTTM": None,
            "OperatingMargins(info)": None,
            "ROE(info)": None,
            "EV_EBITDA": None,
            "PE": None,
            "PEG": None,
            "FCF_Yield": None,
            "PB": None,
            "DivYield": None,
            "PayoutRatio": None,
        }


def build_details_cache():
    """
    ìˆ˜ì •ëœ ìºì‹œ ë¹Œë“œ í•¨ìˆ˜ - ì»¬ëŸ¼ ì¤‘ë³µ ë¬¸ì œ í•´ê²°
    """
    source = CONFIG["UNIVERSE_SOURCE"]
    tickers = load_universe()

    # OHLCV ë¼ì´íŠ¸ ì§€í‘œ ìˆ˜ì§‘ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    TA, PX, VOL, ok = preload_ohlcv_light(
        tickers,
        period=CONFIG["PRELOAD_PERIOD"],
        chunk=CONFIG["PRELOAD_CHUNK"],
        batch_retries=CONFIG["BATCH_RETRIES"],
        single_retries=CONFIG["SINGLE_RETRIES"],
        workers=CONFIG["FALLBACK_MAX_WORKERS"],
        threads=CONFIG["YF_THREADS"],
        sleep=CONFIG["SLEEP_SEC"]
    )

    if not ok:
        raise RuntimeError("OHLCV ë¼ì´íŠ¸ í”„ë¦¬ë¡œë“œ ì‹¤íŒ¨(ë¹ˆ ê²°ê³¼)")

    # ë¼ì´íŠ¸ í‘œ ìƒì„±
    lite_rows = []
    for t in tickers:
        tta = TA.get(t, {})
        price = PX.get(t)
        avg20 = VOL.get(t)
        if price is None or avg20 is None:
            continue
        dollar_vol = price * avg20
        row = {
            "Ticker": t,
            "Price": round(price, 2),
            "DollarVol($M)": round(dollar_vol / 1_000_000, 2),
            "SMA20": tta.get("sma20"),
            "SMA50": tta.get("sma50"),
            "ATR_PCT": tta.get("atr_pct"),
            "RVOL": tta.get("rvol"),
            "RET5": tta.get("ret5"),
            "RET20": tta.get("ret20"),
        }
        lite_rows.append(row)

    lite_df = pd.DataFrame(lite_rows)
    if lite_df.empty:
        raise RuntimeError("ë¼ì´íŠ¸ ì§€í‘œ í‘œê°€ ë¹„ì–´ ìˆìŒ")

    # ìƒì„¸ í˜¸ì¶œ ëŒ€ìƒ ì„ ì • - ìˆ˜ì •ëœ ë¶€ë¶„
    lite_df["_pass_light_generic"] = lite_df.apply(
        lambda r: pass_light_generic(r["Price"], r["DollarVol($M)"] * 1_000_000), axis=1
    )

    # ë¼ì´íŠ¸ í•„í„° í†µê³¼í•œ ì¢…ëª© ì¤‘ì—ì„œë§Œ ìƒìœ„ Kê°œ ì„ ì •
    passed_tickers = lite_df[lite_df["_pass_light_generic"]]
    print(f"ë¼ì´íŠ¸ í•„í„° í†µê³¼: {len(passed_tickers)}ê°œ")

    cand = passed_tickers.sort_values("DollarVol($M)", ascending=False).head(CONFIG["DETAILED_TOP_K"])
    print(f"ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ëŒ€ìƒ: {len(cand)}ê°œ")
    print(f"[ìƒì„¸ë°ì´í„°] í›„ë³´: {len(cand)} / ë¼ì´íŠ¸ ì´ê³„: {len(lite_df)}")

    # ìƒì„¸ ì¬ë¬´ ìˆ˜ì§‘
    detail_rows = []
    success_count = 0

    for i, (t, row) in enumerate(cand.set_index("Ticker").iterrows(), start=1):
        try:
            rec = fetch_details_for_ticker(
                t,
                price=row["Price"],
                avg_vol=(row["DollarVol($M)"] * 1_000_000) / max(1e-9, row["Price"])
            )

            # ë¼ì´íŠ¸ í•„ë“œ ë³‘í•©
            rec.update({
                "SMA20": row.get("SMA20"),
                "SMA50": row.get("SMA50"),
                "ATR_PCT": row.get("ATR_PCT"),
                "RVOL": row.get("RVOL"),
                "RET5": row.get("RET5"),
                "RET20": row.get("RET20"),
            })
            detail_rows.append(rec)
            success_count += 1

        except Exception as e:
            print(f"ì¢…ëª© {t} ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            continue

        if (i % 50) == 0:
            print(f"  - {i}/{len(cand)} ì™„ë£Œ (ì„±ê³µ: {success_count})")

        time.sleep(0.05 + random.random() * 0.05)

    print(f"[ìƒì„¸ë°ì´í„°] ìµœì¢… ìˆ˜ì§‘: {success_count}/{len(cand)} ì¢…ëª©")

    # ê²°ê³¼ ë³‘í•© - ë¼ì´íŠ¸ í•„í„° í†µê³¼í•œ ì „ì²´ ì¢…ëª©ê³¼ ìƒì„¸ ë°ì´í„° ë³‘í•©
    details_df = pd.DataFrame(detail_rows)

    # ğŸ”¥ ì¤‘ìš”: ë¼ì´íŠ¸ í•„í„° í†µê³¼í•œ ì „ì²´ ì¢…ëª©ê³¼ ìƒì„¸ ë°ì´í„° LEFT JOIN
    out = pd.merge(
        passed_tickers.drop(columns=["_pass_light_generic"]),
        details_df,
        on="Ticker",
        how="left"  # ë¼ì´íŠ¸ í•„í„° í†µê³¼í•œ ëª¨ë“  ì¢…ëª©ì€ ìœ ì§€
    )

    print(f"ìµœì¢… CSV í–‰ ìˆ˜: {len(out)} (ë¼ì´íŠ¸ í•„í„° í†µê³¼: {len(passed_tickers)})")

    # ë°©ë²• 2: ë˜ëŠ” lite_dfì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ìœ ì§€í•˜ë©´ì„œ details_dfì˜ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
    # out = lite_df.drop(columns=["_pass_light_generic"]).copy()
    # for col in details_df.columns:
    #     if col != "Ticker":
    #         out[col] = out["Ticker"].map(details_df.set_index("Ticker")[col])

    # ë°ì´í„° íƒ€ì… ì •ë¦¬
    for c in ["RevYoY", "OpMarginTTM", "OperatingMargins(info)", "ROE(info)", "FCF_Yield", "DivYield"]:
        if c in out.columns:
            out[c] = pd.to_numeric(out[c], errors='coerce')

    out["CreatedAtUTC"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    out["Source"] = source

    # ì €ì¥
    base = CONFIG["OUT_BASENAME"].strip() or f"details_cache_{source}"
    csv_path = f"{base}.csv"
    out.to_csv(csv_path, index=False)
    print(f"[ìºì‹œ] ì €ì¥ ì™„ë£Œ: {csv_path} (í–‰: {len(out)})")

    if CONFIG["INCLUDE_EXCEL"]:
        try:
            xlsx_path = f"{base}.xlsx"
            out.to_excel(xlsx_path, index=False)
            print(f"[ìºì‹œ] ì—‘ì…€ ì €ì¥: {xlsx_path}")
        except Exception as e:
            print(f"[ìºì‹œ] ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")

    return out

# ============== ë¼ì´íŠ¸ ì»· í•¨ìˆ˜(ìºì‹œ ë‹¨ê³„ì—ì„œ í›„ë³´ ì¶•ì†Œìš©) ==============
def pass_light_generic(price, dollar_vol):
    if price is None or dollar_vol is None: return False
    return (price >= CONFIG["MIN_PRICE"]) and (dollar_vol >= CONFIG["MIN_DOLLAR_VOLUME"])

if __name__ == "__main__":
    build_details_cache()
